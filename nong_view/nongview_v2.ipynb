{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¤– Nong-View AI ë†ì—…ì˜ìƒë¶„ì„ í”Œë«í¼ - ì™„ì „ í†µí•© ë…¸íŠ¸ë¶ v2.0\n",
    "\n",
    "**í”„ë¡œì íŠ¸ í˜„í™©**: 92% â†’ 98% ì™„ì„± ëª©í‘œ  \n",
    "**í•µì‹¬ ì—…ë°ì´íŠ¸**: POD3(íƒ€ì¼ë§), POD5(ë³‘í•©), POD6(GPKG ë‚´ë³´ë‚´ê¸°) í†µí•©  \n",
    "**ê°œë°œ ë‚ ì§œ**: 2025-10-27  \n",
    "**ë²„ì „**: v2.0\n",
    "\n",
    "## ğŸ“‹ v2.0 ì‹ ê·œ ê¸°ëŠ¥\n",
    "- âœ… v1.0 ëª¨ë“  ê¸°ëŠ¥ í¬í•¨\n",
    "- ğŸ†• POD3: ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ íƒ€ì¼ë§ ì‹œìŠ¤í…œ\n",
    "- ğŸ†• POD5: ì¤‘ë³µ ê²€ì¶œ ê²°ê³¼ ë³‘í•© ì—”ì§„\n",
    "- ğŸ†• POD6: í‘œì¤€ GPKG íŒŒì¼ ë‚´ë³´ë‚´ê¸°\n",
    "- ğŸ†• Crops API ì™„ì „ êµ¬í˜„\n",
    "- ğŸ†• Exports API ì™„ì „ êµ¬í˜„\n",
    "- ğŸ†• Statistics API êµ¬í˜„\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## ğŸ”§ 1. í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (v2.0 ì¶”ê°€ íŒ¨í‚¤ì§€ í¬í•¨)\n",
    "!pip install fastapi uvicorn sqlalchemy alembic psycopg2-binary redis\n",
    "!pip install pydantic[email] python-multipart\n",
    "!pip install rasterio geopandas shapely fiona\n",
    "!pip install ultralytics torch torchvision\n",
    "!pip install pillow opencv-python numpy pandas\n",
    "!pip install python-jose[cryptography] passlib[bcrypt]\n",
    "!pip install pytest pytest-asyncio httpx\n",
    "!pip install rtree tqdm  # POD3, POD5ìš© ì¶”ê°€ íŒ¨í‚¤ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Union, Tuple, Generator\n",
    "from pathlib import Path\n",
    "from uuid import UUID, uuid4\n",
    "import hashlib\n",
    "import time\n",
    "import sqlite3\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì§€ë¦¬ì •ë³´ ì²˜ë¦¬\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.transform import from_bounds\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon, box, mapping\n",
    "from shapely.ops import unary_union\n",
    "from rasterio.crs import CRS\n",
    "from rtree import index\n",
    "\n",
    "# ì›¹ í”„ë ˆì„ì›Œí¬\n",
    "from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, FileResponse\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime, Float, Text, Boolean, ForeignKey, JSON\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session, relationship\n",
    "from sqlalchemy.dialects.postgresql import UUID as PG_UUID\n",
    "\n",
    "# AI/ML\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ (v2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. ì„¤ì • ë° êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì • í´ë˜ìŠ¤ ì •ì˜ (v2.0 í™•ì¥)\n",
    "class Settings:\n",
    "    \"\"\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • v2.0\"\"\"\n",
    "    PROJECT_NAME: str = \"Nong-View API\"\n",
    "    VERSION: str = \"2.0.0\"\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    ENVIRONMENT: str = \"development\"\n",
    "    DEBUG: bool = True\n",
    "    \n",
    "    # ì„œë²„ ì„¤ì •\n",
    "    HOST: str = \"127.0.0.1\"\n",
    "    PORT: int = 8000\n",
    "    \n",
    "    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •\n",
    "    DATABASE_URL: str = \"sqlite:///./nongview_v2.db\"\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥ ì„¤ì •\n",
    "    UPLOAD_PATH: str = \"./uploads\"\n",
    "    CROP_PATH: str = \"./crops\"\n",
    "    TILE_PATH: str = \"./tiles\"  # v2.0 ì¶”ê°€\n",
    "    EXPORT_PATH: str = \"./exports\"\n",
    "    MAX_FILE_SIZE: int = 2 * 1024 * 1024 * 1024  # 2GB\n",
    "    \n",
    "    # AI ëª¨ë¸ ì„¤ì •\n",
    "    MODEL_PATH: str = \"./models\"\n",
    "    MAX_WORKERS: int = 4\n",
    "    \n",
    "    # íƒ€ì¼ë§ ì„¤ì • (v2.0 ì¶”ê°€)\n",
    "    TILE_SIZE: int = 640\n",
    "    TILE_OVERLAP: float = 0.2\n",
    "    \n",
    "    # ë³‘í•© ì„¤ì • (v2.0 ì¶”ê°€)\n",
    "    IOU_THRESHOLD: float = 0.5\n",
    "    CONFIDENCE_THRESHOLD: float = 0.3\n",
    "    \n",
    "    # GPKG ë‚´ë³´ë‚´ê¸° ì„¤ì • (v2.0 ì¶”ê°€)\n",
    "    OUTPUT_CRS: str = \"EPSG:5186\"  # UTM-K\n",
    "    PRIVACY_MASK: bool = True\n",
    "    \n",
    "    # ë³´ì•ˆ ì„¤ì •\n",
    "    SECRET_KEY: str = \"dev-secret-key-change-in-production\"\n",
    "    ALGORITHM: str = \"HS256\"\n",
    "    ACCESS_TOKEN_EXPIRE_MINUTES: int = 1440\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for path in [settings.UPLOAD_PATH, settings.CROP_PATH, settings.TILE_PATH, \n",
    "             settings.EXPORT_PATH, settings.MODEL_PATH]:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… ì„¤ì • ì™„ë£Œ (v2.0)\")\n",
    "print(f\"ğŸ“ ì—…ë¡œë“œ ê²½ë¡œ: {settings.UPLOAD_PATH}\")\n",
    "print(f\"ğŸ”² íƒ€ì¼ ê²½ë¡œ: {settings.TILE_PATH}\")\n",
    "print(f\"ğŸ“¦ ë‚´ë³´ë‚´ê¸° ê²½ë¡œ: {settings.EXPORT_PATH}\")\n",
    "print(f\"ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: {settings.DATABASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "database_section",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ 3. ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜ (v2.0 í™•ì¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "database_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy ì„¤ì •\n",
    "engine = create_engine(\n",
    "    settings.DATABASE_URL,\n",
    "    connect_args={\"check_same_thread\": False} if \"sqlite\" in settings.DATABASE_URL else {}\n",
    ")\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜ (v2.0 í™•ì¥)\n",
    "class User(Base):\n",
    "    \"\"\"ì‚¬ìš©ì ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"users\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    username = Column(String(50), unique=True, index=True, nullable=False)\n",
    "    email = Column(String(100), unique=True, index=True, nullable=False)\n",
    "    hashed_password = Column(String(100), nullable=False)\n",
    "    is_active = Column(Boolean, default=True)\n",
    "    is_superuser = Column(Boolean, default=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    images = relationship(\"Image\", back_populates=\"owner\")\n",
    "    analyses = relationship(\"Analysis\", back_populates=\"owner\")\n",
    "\n",
    "\n",
    "class Image(Base):\n",
    "    \"\"\"ì´ë¯¸ì§€ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"images\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    file_path = Column(String(500), nullable=False)\n",
    "    file_size = Column(Integer, nullable=False)\n",
    "    format = Column(String(20), nullable=False)\n",
    "    status = Column(String(20), default=\"uploading\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°\n",
    "    description = Column(Text)\n",
    "    region_name = Column(String(100))\n",
    "    capture_date = Column(DateTime)\n",
    "    drone_model = Column(String(100))\n",
    "    camera_model = Column(String(100))\n",
    "    altitude = Column(Float)\n",
    "    overlap = Column(Float)\n",
    "    tags = Column(JSON)  # List[str]\n",
    "    \n",
    "    # ì§€ë¦¬ ì •ë³´\n",
    "    crs = Column(String(50))\n",
    "    bounds = Column(JSON)  # {\"minx\": float, \"miny\": float, \"maxx\": float, \"maxy\": float}\n",
    "    resolution = Column(Float)\n",
    "    width = Column(Integer)\n",
    "    height = Column(Integer)\n",
    "    bands = Column(Integer)\n",
    "    \n",
    "    # ì†Œìœ ì ë° ì‹œê°„\n",
    "    owner_id = Column(String, ForeignKey(\"users.id\"), nullable=False)\n",
    "    uploaded_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    owner = relationship(\"User\", back_populates=\"images\")\n",
    "    analyses = relationship(\"Analysis\", back_populates=\"image\")\n",
    "    crop_results = relationship(\"CropResult\", back_populates=\"image\")\n",
    "    tiles = relationship(\"Tile\", back_populates=\"image\")  # v2.0 ì¶”ê°€\n",
    "\n",
    "\n",
    "class Analysis(Base):\n",
    "    \"\"\"ë¶„ì„ ì‘ì—… ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"analyses\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    name = Column(String(200), nullable=False)\n",
    "    description = Column(Text)\n",
    "    status = Column(String(20), default=\"pending\")  # pending, running, completed, error\n",
    "    \n",
    "    # ë¶„ì„ ì„¤ì •\n",
    "    analysis_type = Column(String(50), nullable=False)  # crop, facility, landuse\n",
    "    model_version = Column(String(50))\n",
    "    confidence_threshold = Column(Float, default=0.5)\n",
    "    use_tiling = Column(Boolean, default=False)  # v2.0 ì¶”ê°€\n",
    "    config = Column(JSON)  # ë¶„ì„ ì„¤ì •\n",
    "    \n",
    "    # ê²°ê³¼\n",
    "    total_detections = Column(Integer, default=0)\n",
    "    total_tiles = Column(Integer, default=0)  # v2.0 ì¶”ê°€\n",
    "    processing_time = Column(Float)\n",
    "    error_message = Column(Text)\n",
    "    result_data = Column(JSON)  # ë¶„ì„ ê²°ê³¼ ìš”ì•½\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image_id = Column(String, ForeignKey(\"images.id\"), nullable=False)\n",
    "    owner_id = Column(String, ForeignKey(\"users.id\"), nullable=False)\n",
    "    \n",
    "    # ì‹œê°„\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    started_at = Column(DateTime)\n",
    "    completed_at = Column(DateTime)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image = relationship(\"Image\", back_populates=\"analyses\")\n",
    "    owner = relationship(\"User\", back_populates=\"analyses\")\n",
    "    crop_results = relationship(\"CropResult\", back_populates=\"analysis\")\n",
    "    exports = relationship(\"Export\", back_populates=\"analysis\")\n",
    "    tiles = relationship(\"Tile\", back_populates=\"analysis\")  # v2.0 ì¶”ê°€\n",
    "\n",
    "\n",
    "class Tile(Base):  # v2.0 ì‹ ê·œ ì¶”ê°€\n",
    "    \"\"\"íƒ€ì¼ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"tiles\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    tile_path = Column(String(500), nullable=False)\n",
    "    tile_size = Column(Integer, nullable=False)\n",
    "    \n",
    "    # ìœ„ì¹˜ ì •ë³´\n",
    "    row = Column(Integer, nullable=False)\n",
    "    col = Column(Integer, nullable=False)\n",
    "    x_offset = Column(Integer, nullable=False)\n",
    "    y_offset = Column(Integer, nullable=False)\n",
    "    \n",
    "    # ì¢Œí‘œ ì •ë³´\n",
    "    bounds = Column(JSON)  # ì§€ë¦¬ì  ì¢Œí‘œ\n",
    "    pixel_bounds = Column(JSON)  # í”½ì…€ ì¢Œí‘œ\n",
    "    \n",
    "    # AI ë¶„ì„ ê²°ê³¼\n",
    "    detection_count = Column(Integer, default=0)\n",
    "    detection_data = Column(JSON)  # ê²€ì¶œ ê²°ê³¼\n",
    "    processed = Column(Boolean, default=False)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image_id = Column(String, ForeignKey(\"images.id\"), nullable=False)\n",
    "    analysis_id = Column(String, ForeignKey(\"analyses.id\"))\n",
    "    \n",
    "    # ì‹œê°„\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    processed_at = Column(DateTime)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image = relationship(\"Image\", back_populates=\"tiles\")\n",
    "    analysis = relationship(\"Analysis\", back_populates=\"tiles\")\n",
    "\n",
    "\n",
    "class CropResult(Base):\n",
    "    \"\"\"í¬ë¡œí•‘ ê²°ê³¼ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"crop_results\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    pnu = Column(String(19), nullable=False, index=True)  # í•„ì§€ê³ ìœ ë²ˆí˜¸\n",
    "    crop_path = Column(String(500), nullable=False)\n",
    "    crop_size = Column(Integer)\n",
    "    \n",
    "    # ì§€ì˜¤ë©”íŠ¸ë¦¬ ì •ë³´\n",
    "    geometry_type = Column(String(50))\n",
    "    area = Column(Float)\n",
    "    perimeter = Column(Float)\n",
    "    bounds = Column(JSON)\n",
    "    \n",
    "    # AI ë¶„ì„ ê²°ê³¼\n",
    "    detected_class = Column(String(50))\n",
    "    confidence = Column(Float)\n",
    "    detection_count = Column(Integer, default=0)\n",
    "    detection_data = Column(JSON)  # ìƒì„¸ ê²€ì¶œ ê²°ê³¼\n",
    "    merged_from_tiles = Column(Boolean, default=False)  # v2.0 ì¶”ê°€\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image_id = Column(String, ForeignKey(\"images.id\"), nullable=False)\n",
    "    analysis_id = Column(String, ForeignKey(\"analyses.id\"))\n",
    "    \n",
    "    # ì‹œê°„\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image = relationship(\"Image\", back_populates=\"crop_results\")\n",
    "    analysis = relationship(\"Analysis\", back_populates=\"crop_results\")\n",
    "\n",
    "\n",
    "class Export(Base):\n",
    "    \"\"\"ë‚´ë³´ë‚´ê¸° ì‘ì—… ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"exports\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    name = Column(String(200), nullable=False)\n",
    "    export_type = Column(String(50), nullable=False)  # gpkg, geojson, csv, shapefile\n",
    "    status = Column(String(20), default=\"pending\")  # pending, processing, completed, error\n",
    "    \n",
    "    # íŒŒì¼ ì •ë³´\n",
    "    file_path = Column(String(500))\n",
    "    file_size = Column(Integer)\n",
    "    download_count = Column(Integer, default=0)\n",
    "    \n",
    "    # ë‚´ë³´ë‚´ê¸° ì„¤ì •\n",
    "    include_geometry = Column(Boolean, default=True)\n",
    "    include_metadata = Column(Boolean, default=True)\n",
    "    privacy_applied = Column(Boolean, default=False)  # v2.0 ì¶”ê°€\n",
    "    layer_count = Column(Integer, default=1)  # v2.0 ì¶”ê°€\n",
    "    filter_config = Column(JSON)  # í•„í„° ì„¤ì •\n",
    "    \n",
    "    # ì˜¤ë¥˜ ì •ë³´\n",
    "    error_message = Column(Text)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    analysis_id = Column(String, ForeignKey(\"analyses.id\"), nullable=False)\n",
    "    \n",
    "    # ì‹œê°„\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    completed_at = Column(DateTime)\n",
    "    expires_at = Column(DateTime)  # ë‹¤ìš´ë¡œë“œ ë§Œë£Œì¼\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    analysis = relationship(\"Analysis\", back_populates=\"exports\")\n",
    "\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„± ì™„ë£Œ (v2.0)\")\n",
    "print(f\"ğŸ“Š ìƒì„±ëœ í…Œì´ë¸”: {list(Base.metadata.tables.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "database_session",
   "metadata": {},
   "source": [
    "## ğŸ”— 4. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±\n",
    "def get_db():\n",
    "    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„±\"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© ì‚¬ìš©ì ìƒì„±\n",
    "def create_test_user(db: Session):\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ìš© ì‚¬ìš©ì ìƒì„±\"\"\"\n",
    "    existing_user = db.query(User).filter(User.username == \"testuser\").first()\n",
    "    if not existing_user:\n",
    "        test_user = User(\n",
    "            username=\"testuser\",\n",
    "            email=\"test@example.com\",\n",
    "            hashed_password=\"hashed_password_here\",\n",
    "            is_active=True\n",
    "        )\n",
    "        db.add(test_user)\n",
    "        db.commit()\n",
    "        db.refresh(test_user)\n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ìƒì„±: {test_user.id}\")\n",
    "        return test_user\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ì¡´ì¬: {existing_user.id}\")\n",
    "        return existing_user\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ìƒì„±\n",
    "with SessionLocal() as db:\n",
    "    test_user = create_test_user(db)\n",
    "    \n",
    "print(\"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pod_integration",
   "metadata": {},
   "source": [
    "## ğŸ”Œ 5. POD ëª¨ë“ˆ í†µí•© (v2.0 - ì „ì²´ ëª¨ë“ˆ)\n",
    "### POD1: ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pod1_data_ingestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD1: ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬ ëª¨ë“ˆ (v1.0ê³¼ ë™ì¼)\n",
    "class DataRegistry:\n",
    "    \"\"\"ì¤‘ì•™ ë°ì´í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def extract_image_metadata(self, file_path: Path) -> dict:\n",
    "        \"\"\"ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "        try:\n",
    "            with rasterio.open(file_path) as src:\n",
    "                bounds = {\n",
    "                    \"minx\": src.bounds.left,\n",
    "                    \"miny\": src.bounds.bottom,\n",
    "                    \"maxx\": src.bounds.right,\n",
    "                    \"maxy\": src.bounds.top\n",
    "                }\n",
    "                \n",
    "                # í•´ìƒë„ ê³„ì‚°\n",
    "                res_x = (src.bounds.right - src.bounds.left) / src.width\n",
    "                res_y = (src.bounds.top - src.bounds.bottom) / src.height\n",
    "                resolution = (res_x + res_y) / 2\n",
    "                \n",
    "                metadata = {\n",
    "                    \"crs\": str(src.crs) if src.crs else \"EPSG:5186\",\n",
    "                    \"resolution\": resolution,\n",
    "                    \"bounds\": bounds,\n",
    "                    \"width\": src.width,\n",
    "                    \"height\": src.height,\n",
    "                    \"bands\": src.count,\n",
    "                    \"format\": src.driver\n",
    "                }\n",
    "                \n",
    "                return metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_file(self, upload_file: UploadFile) -> tuple[str, int]:\n",
    "        \"\"\"íŒŒì¼ ì €ì¥\"\"\"\n",
    "        file_id = str(uuid4())\n",
    "        file_extension = Path(upload_file.filename).suffix\n",
    "        file_path = self.storage_path / f\"{file_id}{file_extension}\"\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        content = upload_file.file.read()\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return str(file_path), len(content)\n",
    "\n",
    "# ì „ì—­ ë°ì´í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¸ìŠ¤í„´ìŠ¤\n",
    "data_registry = DataRegistry(settings.UPLOAD_PATH)\n",
    "\n",
    "print(\"âœ… POD1 ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ í†µí•© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pod2_cropping",
   "metadata": {},
   "source": [
    "### POD2: í¬ë¡œí•‘ ì—”ì§„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pod2_cropping_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD2: í¬ë¡œí•‘ ì—”ì§„ (v1.0ê³¼ ë™ì¼)\n",
    "class CroppingEngine:\n",
    "    \"\"\"ROI ì¶”ì¶œ ë° í¬ë¡œí•‘ ì—”ì§„\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def crop_by_shapes(self, image_path: str, shapefile_path: str) -> list:\n",
    "        \"\"\"Shapefile ê¸°ë°˜ ì´ë¯¸ì§€ í¬ë¡œí•‘\"\"\"\n",
    "        try:\n",
    "            # Shapefile ë¡œë“œ\n",
    "            gdf = gpd.read_file(shapefile_path)\n",
    "            \n",
    "            results = []\n",
    "            \n",
    "            with rasterio.open(image_path) as src:\n",
    "                for idx, row in gdf.iterrows():\n",
    "                    pnu = row.get('PNU', f'unknown_{idx}').zfill(19)\n",
    "                    geometry = row.geometry\n",
    "                    \n",
    "                    # í¬ë¡œí•‘ ìˆ˜í–‰\n",
    "                    try:\n",
    "                        # ì§€ì˜¤ë©”íŠ¸ë¦¬ë¡œ ë§ˆìŠ¤í‚¹\n",
    "                        out_image, out_transform = rasterio.mask.mask(\n",
    "                            src, [geometry], crop=True\n",
    "                        )\n",
    "                        \n",
    "                        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "                        crop_id = str(uuid4())\n",
    "                        crop_filename = f\"crop_{crop_id}_{pnu}.tif\"\n",
    "                        crop_path = self.output_dir / crop_filename\n",
    "                        \n",
    "                        # í¬ë¡­ëœ ì´ë¯¸ì§€ ì €ì¥\n",
    "                        out_meta = src.meta\n",
    "                        out_meta.update({\n",
    "                            \"driver\": \"GTiff\",\n",
    "                            \"height\": out_image.shape[1],\n",
    "                            \"width\": out_image.shape[2],\n",
    "                            \"transform\": out_transform\n",
    "                        })\n",
    "                        \n",
    "                        with rasterio.open(crop_path, \"w\", **out_meta) as dest:\n",
    "                            dest.write(out_image)\n",
    "                        \n",
    "                        # ê²°ê³¼ ì •ë³´\n",
    "                        result = {\n",
    "                            \"crop_id\": crop_id,\n",
    "                            \"pnu\": pnu,\n",
    "                            \"crop_path\": str(crop_path),\n",
    "                            \"crop_size\": crop_path.stat().st_size,\n",
    "                            \"geometry_type\": geometry.geom_type,\n",
    "                            \"area\": geometry.area,\n",
    "                            \"perimeter\": geometry.length,\n",
    "                            \"bounds\": {\n",
    "                                \"minx\": geometry.bounds[0],\n",
    "                                \"miny\": geometry.bounds[1],\n",
    "                                \"maxx\": geometry.bounds[2],\n",
    "                                \"maxy\": geometry.bounds[3]\n",
    "                            }\n",
    "                        }\n",
    "                        \n",
    "                        results.append(result)\n",
    "                        logger.info(f\"í¬ë¡œí•‘ ì™„ë£Œ: {pnu}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"í¬ë¡œí•‘ ì‹¤íŒ¨ - PNU {pnu}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"í¬ë¡œí•‘ ì—”ì§„ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "# ì „ì—­ í¬ë¡œí•‘ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "cropping_engine = CroppingEngine(settings.CROP_PATH)\n",
    "\n",
    "print(\"âœ… POD2 í¬ë¡œí•‘ ì—”ì§„ í†µí•© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pod3_tiling",
   "metadata": {},
   "source": [
    "### POD3: íƒ€ì¼ë§ ì‹œìŠ¤í…œ (v2.0 ì‹ ê·œ ì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pod3_tiling_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD3: íƒ€ì¼ë§ ì‹œìŠ¤í…œ (v2.0 ì‹ ê·œ)\n",
    "class TilingEngine:\n",
    "    \"\"\"ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ íƒ€ì¼ë§ ì—”ì§„\"\"\"\n",
    "    \n",
    "    def __init__(self, tile_size: int = None, overlap: float = None, output_dir: str = None):\n",
    "        self.tile_size = tile_size or settings.TILE_SIZE\n",
    "        self.overlap = overlap or settings.TILE_OVERLAP\n",
    "        self.output_dir = Path(output_dir or settings.TILE_PATH)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.stride = int(self.tile_size * (1 - self.overlap))\n",
    "    \n",
    "    def calculate_grid_size(self, image_width: int, image_height: int) -> tuple[int, int]:\n",
    "        \"\"\"ê·¸ë¦¬ë“œ í¬ê¸° ê³„ì‚°\"\"\"\n",
    "        cols = (image_width - self.tile_size) // self.stride + 1\n",
    "        rows = (image_height - self.tile_size) // self.stride + 1\n",
    "        \n",
    "        # ë‚˜ë¨¸ì§€ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì¶”ê°€ íƒ€ì¼ ìƒì„±\n",
    "        if (image_width - self.tile_size) % self.stride > 0:\n",
    "            cols += 1\n",
    "        if (image_height - self.tile_size) % self.stride > 0:\n",
    "            rows += 1\n",
    "            \n",
    "        return rows, cols\n",
    "    \n",
    "    def generate_tiles(self, image_path: str, analysis_id: str) -> list:\n",
    "        \"\"\"ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í• \"\"\"\n",
    "        try:\n",
    "            with rasterio.open(image_path) as src:\n",
    "                image_width = src.width\n",
    "                image_height = src.height\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "                \n",
    "                rows, cols = self.calculate_grid_size(image_width, image_height)\n",
    "                \n",
    "                logger.info(f\"íƒ€ì¼ë§ ì‹œì‘: {image_path} -> {rows}x{cols} = {rows*cols}ê°œ íƒ€ì¼\")\n",
    "                \n",
    "                tiles = []\n",
    "                \n",
    "                for row in tqdm(range(rows), desc=\"íƒ€ì¼ë§ ì§„í–‰\"):\n",
    "                    for col in range(cols):\n",
    "                        # í”½ì…€ ì¢Œí‘œ ê³„ì‚°\n",
    "                        x_start = col * self.stride\n",
    "                        y_start = row * self.stride\n",
    "                        x_end = min(x_start + self.tile_size, image_width)\n",
    "                        y_end = min(y_start + self.tile_size, image_height)\n",
    "                        \n",
    "                        # ì‹¤ì œ íƒ€ì¼ í¬ê¸°\n",
    "                        actual_width = x_end - x_start\n",
    "                        actual_height = y_end - y_start\n",
    "                        \n",
    "                        # ë„ˆë¬´ ì‘ì€ íƒ€ì¼ì€ ì œì™¸\n",
    "                        if actual_width < self.tile_size * 0.5 or actual_height < self.tile_size * 0.5:\n",
    "                            continue\n",
    "                        \n",
    "                        # ìœˆë„ìš° ìƒì„±\n",
    "                        window = Window(x_start, y_start, actual_width, actual_height)\n",
    "                        \n",
    "                        # ì§€ë¦¬ì  ì¢Œí‘œ ê³„ì‚°\n",
    "                        bounds = rasterio.windows.bounds(window, transform)\n",
    "                        \n",
    "                        # íƒ€ì¼ ID ìƒì„±\n",
    "                        tile_id = str(uuid4())\n",
    "                        tile_filename = f\"tile_{tile_id}_{row}_{col}.tif\"\n",
    "                        tile_path = self.output_dir / analysis_id / tile_filename\n",
    "                        tile_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        \n",
    "                        # íƒ€ì¼ ì´ë¯¸ì§€ ì €ì¥\n",
    "                        tile_data = src.read(window=window)\n",
    "                        tile_transform = rasterio.windows.transform(window, transform)\n",
    "                        \n",
    "                        tile_meta = src.meta.copy()\n",
    "                        tile_meta.update({\n",
    "                            'height': actual_height,\n",
    "                            'width': actual_width,\n",
    "                            'transform': tile_transform\n",
    "                        })\n",
    "                        \n",
    "                        with rasterio.open(tile_path, 'w', **tile_meta) as dst:\n",
    "                            dst.write(tile_data)\n",
    "                        \n",
    "                        # íƒ€ì¼ ì •ë³´ ìƒì„±\n",
    "                        tile_info = {\n",
    "                            \"tile_id\": tile_id,\n",
    "                            \"tile_path\": str(tile_path),\n",
    "                            \"row\": row,\n",
    "                            \"col\": col,\n",
    "                            \"x_offset\": x_start,\n",
    "                            \"y_offset\": y_start,\n",
    "                            \"bounds\": {\n",
    "                                \"minx\": bounds.left,\n",
    "                                \"miny\": bounds.bottom,\n",
    "                                \"maxx\": bounds.right,\n",
    "                                \"maxy\": bounds.top\n",
    "                            },\n",
    "                            \"pixel_bounds\": {\n",
    "                                \"x_start\": x_start,\n",
    "                                \"y_start\": y_start,\n",
    "                                \"x_end\": x_end,\n",
    "                                \"y_end\": y_end\n",
    "                            },\n",
    "                            \"tile_size\": actual_width * actual_height\n",
    "                        }\n",
    "                        \n",
    "                        tiles.append(tile_info)\n",
    "                \n",
    "                logger.info(f\"íƒ€ì¼ë§ ì™„ë£Œ: {len(tiles)}ê°œ íƒ€ì¼ ìƒì„±\")\n",
    "                return tiles\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"íƒ€ì¼ë§ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "\n",
    "# ì „ì—­ íƒ€ì¼ë§ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "tiling_engine = TilingEngine()\n",
    "\n",
    "print(\"âœ… POD3 íƒ€ì¼ë§ ì—”ì§„ í†µí•© ì™„ë£Œ (v2.0 ì‹ ê·œ)\")\n",
    "print(f\"ğŸ”² íƒ€ì¼ í¬ê¸°: {tiling_engine.tile_size}px\")\n",
    "print(f\"ğŸ“ ê²¹ì¹¨ë¥ : {tiling_engine.overlap*100}%\")\n",
    "print(f\"ğŸ“ íƒ€ì¼ ì €ì¥ì†Œ: {tiling_engine.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pod4_ai",
   "metadata": {},
   "source": [
    "### POD4: AI ì¶”ë¡  ì—”ì§„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pod4_ai_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD4: AI ì¶”ë¡  ì—”ì§„ (v1.0 ê¸°ë°˜, v2.0 í™•ì¥)\n",
    "class AIInferenceEngine:\n",
    "    \"\"\"YOLOv11 ê¸°ë°˜ AI ì¶”ë¡  ì—”ì§„ v2.0\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"AI ì—”ì§„ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ë§¤í•‘\n",
    "        self.class_mappings = {\n",
    "            'crop': {\n",
    "                0: 'IRG',      # ì´íƒˆë¦¬ì•ˆ ë¼ì´ê·¸ë¼ìŠ¤\n",
    "                1: 'BARLEY',   # ë³´ë¦¬\n",
    "                2: 'WHEAT',    # ë°€\n",
    "                3: 'CORN_SILAGE',  # ì˜¥ìˆ˜ìˆ˜ì‚¬ì¼ë¦¬ì§€\n",
    "                4: 'HAY',      # ê±´ì´ˆ\n",
    "                5: 'UNKNOWN'   # ë¯¸ë¶„ë¥˜\n",
    "            },\n",
    "            'facility': {\n",
    "                0: 'GREENHOUSE_SINGLE',  # ë‹¨ë™ ë¹„ë‹í•˜ìš°ìŠ¤\n",
    "                1: 'GREENHOUSE_MULTI',   # ì—°ë™ ë¹„ë‹í•˜ìš°ìŠ¤\n",
    "                2: 'STORAGE',            # ì €ì¥ì‹œì„¤\n",
    "                3: 'LIVESTOCK',          # ì¶•ì‚¬\n",
    "                4: 'SILO',              # ì‚¬ì¼ë¡œ\n",
    "                5: 'UNKNOWN'            # ë¯¸ë¶„ë¥˜\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def load_model(self, model_type: str, model_path: str = None):\n",
    "        \"\"\"YOLO ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            if model_path and Path(model_path).exists():\n",
    "                model = YOLO(model_path)\n",
    "            else:\n",
    "                # ê¸°ë³¸ YOLOv8 ëª¨ë¸ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)\n",
    "                model = YOLO('yolov8n.pt')\n",
    "                logger.warning(f\"ì‚¬ìš©ì ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {model_type}\")\n",
    "            \n",
    "            self.models[model_type] = model\n",
    "            logger.info(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_type}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - {model_type}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, image_path: str, model_type: str, confidence: float = 0.5) -> dict:\n",
    "        \"\"\"ì´ë¯¸ì§€ ì˜ˆì¸¡ ìˆ˜í–‰\"\"\"\n",
    "        try:\n",
    "            if model_type not in self.models:\n",
    "                raise ValueError(f\"ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ: {model_type}\")\n",
    "            \n",
    "            model = self.models[model_type]\n",
    "            \n",
    "            # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "            results = model(image_path, conf=confidence, device=self.device)\n",
    "            \n",
    "            detections = []\n",
    "            \n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None:\n",
    "                    for box in boxes:\n",
    "                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                        confidence_score = box.conf[0].item()\n",
    "                        class_id = int(box.cls[0].item())\n",
    "                        \n",
    "                        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘\n",
    "                        class_name = self.class_mappings.get(model_type, {}).get(\n",
    "                            class_id, f\"class_{class_id}\"\n",
    "                        )\n",
    "                        \n",
    "                        detection = {\n",
    "                            \"class_id\": class_id,\n",
    "                            \"class_name\": class_name,\n",
    "                            \"confidence\": confidence_score,\n",
    "                            \"bbox\": [x1, y1, x2, y2],\n",
    "                            \"area\": (x2 - x1) * (y2 - y1)\n",
    "                        }\n",
    "                        \n",
    "                        detections.append(detection)\n",
    "            \n",
    "            return {\n",
    "                \"detections\": detections,\n",
    "                \"detection_count\": len(detections),\n",
    "                \"model_type\": model_type,\n",
    "                \"confidence_threshold\": confidence\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_batch(self, image_paths: list, model_type: str, confidence: float = 0.5) -> list:\n",
    "        \"\"\"ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰ (v2.0 í™•ì¥)\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš©\n",
    "        with ThreadPoolExecutor(max_workers=settings.MAX_WORKERS) as executor:\n",
    "            # ë¹„ë™ê¸° ì‘ì—… ì œì¶œ\n",
    "            future_to_path = {\n",
    "                executor.submit(self.predict, path, model_type, confidence): path \n",
    "                for path in image_paths\n",
    "            }\n",
    "            \n",
    "            # ê²°ê³¼ ìˆ˜ì§‘\n",
    "            for future in tqdm(future_to_path, desc=\"AI ì¶”ë¡  ì§„í–‰\"):\n",
    "                image_path = future_to_path[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    result['image_path'] = image_path\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨ - {image_path}: {e}\")\n",
    "                    results.append({\n",
    "                        'image_path': image_path,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ì „ì—­ AI ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "ai_engine = AIInferenceEngine()\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "try:\n",
    "    ai_engine.load_model('crop')\n",
    "    ai_engine.load_model('facility')\n",
    "    print(\"âœ… POD4 AI ì¶”ë¡  ì—”ì§„ í†µí•© ì™„ë£Œ (v2.0 í™•ì¥)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ê¸°ë³¸ YOLO ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pod5_merging",
   "metadata": {},
   "source": [
    "### POD5: ê²°ê³¼ ë³‘í•© ì—”ì§„ (v2.0 ì‹ ê·œ ì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pod5_merging_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD5: ê²°ê³¼ ë³‘í•© ì—”ì§„ (v2.0 ì‹ ê·œ)\n",
    "class MergeEngine:\n",
    "    \"\"\"íƒ€ì¼ ê¸°ë°˜ ê²€ì¶œ ê²°ê³¼ ë³‘í•© ì—”ì§„\"\"\"\n",
    "    \n",
    "    def __init__(self, iou_threshold: float = None, confidence_threshold: float = None):\n",
    "        self.iou_threshold = iou_threshold or settings.IOU_THRESHOLD\n",
    "        self.confidence_threshold = confidence_threshold or settings.CONFIDENCE_THRESHOLD\n",
    "    \n",
    "    def calculate_iou(self, box1: list, box2: list) -> float:\n",
    "        \"\"\"ë‘ ë°”ìš´ë”© ë°•ìŠ¤ì˜ IoU ê³„ì‚°\"\"\"\n",
    "        x1_1, y1_1, x2_1, y2_1 = box1\n",
    "        x1_2, y1_2, x2_2, y2_2 = box2\n",
    "        \n",
    "        # êµì§‘í•© ì˜ì—­ ê³„ì‚°\n",
    "        x1_i = max(x1_1, x1_2)\n",
    "        y1_i = max(y1_1, y1_2)\n",
    "        x2_i = min(x2_1, x2_2)\n",
    "        y2_i = min(y2_1, y2_2)\n",
    "        \n",
    "        if x2_i <= x1_i or y2_i <= y1_i:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "        \n",
    "        # í•©ì§‘í•© ì˜ì—­ ê³„ì‚°\n",
    "        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def convert_to_global_coords(self, tile_results: list, tile_metadata: list) -> list:\n",
    "        \"\"\"íƒ€ì¼ ì¢Œí‘œë¥¼ ì „ì—­ ì¢Œí‘œë¡œ ë³€í™˜\"\"\"\n",
    "        global_detections = []\n",
    "        \n",
    "        for tile_result in tile_results:\n",
    "            if 'error' in tile_result:\n",
    "                continue\n",
    "                \n",
    "            image_path = tile_result.get('image_path', '')\n",
    "            \n",
    "            # íƒ€ì¼ ë©”íƒ€ë°ì´í„° ì°¾ê¸°\n",
    "            tile_meta = None\n",
    "            for meta in tile_metadata:\n",
    "                if meta['tile_path'] == image_path:\n",
    "                    tile_meta = meta\n",
    "                    break\n",
    "            \n",
    "            if not tile_meta:\n",
    "                logger.warning(f\"íƒ€ì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            x_offset = tile_meta['x_offset']\n",
    "            y_offset = tile_meta['y_offset']\n",
    "            \n",
    "            for detection in tile_result.get('detections', []):\n",
    "                # ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì „ì—­ ì¢Œí‘œë¡œ ë³€í™˜\n",
    "                bbox = detection['bbox']\n",
    "                global_bbox = [\n",
    "                    bbox[0] + x_offset,  # x1\n",
    "                    bbox[1] + y_offset,  # y1\n",
    "                    bbox[2] + x_offset,  # x2\n",
    "                    bbox[3] + y_offset   # y2\n",
    "                ]\n",
    "                \n",
    "                global_detection = {\n",
    "                    'class_id': detection['class_id'],\n",
    "                    'class_name': detection['class_name'],\n",
    "                    'confidence': detection['confidence'],\n",
    "                    'bbox': global_bbox,\n",
    "                    'area': detection['area'],\n",
    "                    'tile_id': tile_meta['tile_id'],\n",
    "                    'source_tile': image_path\n",
    "                }\n",
    "                \n",
    "                global_detections.append(global_detection)\n",
    "        \n",
    "        return global_detections\n",
    "    \n",
    "    def merge_overlapping_detections(self, detections: list) -> list:\n",
    "        \"\"\"ê²¹ì¹˜ëŠ” ê²€ì¶œ ê²°ê³¼ ë³‘í•©\"\"\"\n",
    "        if not detections:\n",
    "            return []\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ê¸°ì¤€ í•„í„°ë§\n",
    "        filtered_detections = [\n",
    "            d for d in detections \n",
    "            if d['confidence'] >= self.confidence_threshold\n",
    "        ]\n",
    "        \n",
    "        # í´ë˜ìŠ¤ë³„ë¡œ ê·¸ë£¹í™”\n",
    "        class_groups = {}\n",
    "        for detection in filtered_detections:\n",
    "            class_name = detection['class_name']\n",
    "            if class_name not in class_groups:\n",
    "                class_groups[class_name] = []\n",
    "            class_groups[class_name].append(detection)\n",
    "        \n",
    "        merged_detections = []\n",
    "        \n",
    "        # í´ë˜ìŠ¤ë³„ë¡œ NMS ì ìš©\n",
    "        for class_name, class_detections in class_groups.items():\n",
    "            # ì‹ ë¢°ë„ ê¸°ì¤€ ì •ë ¬ (ë†’ì€ ìˆœ)\n",
    "            class_detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "            \n",
    "            while class_detections:\n",
    "                # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ê²€ì¶œ ì„ íƒ\n",
    "                best_detection = class_detections.pop(0)\n",
    "                merged_detections.append(best_detection)\n",
    "                \n",
    "                # ê²¹ì¹˜ëŠ” ê²€ì¶œë“¤ ì œê±°\n",
    "                remaining = []\n",
    "                for detection in class_detections:\n",
    "                    iou = self.calculate_iou(best_detection['bbox'], detection['bbox'])\n",
    "                    if iou < self.iou_threshold:\n",
    "                        remaining.append(detection)\n",
    "                    else:\n",
    "                        logger.debug(f\"IoU {iou:.3f}ë¡œ ê²€ì¶œ ë³‘í•©: {class_name}\")\n",
    "                \n",
    "                class_detections = remaining\n",
    "        \n",
    "        logger.info(f\"ë³‘í•© ì™„ë£Œ: {len(detections)} â†’ {len(merged_detections)}ê°œ ê²€ì¶œ\")\n",
    "        return merged_detections\n",
    "    \n",
    "    def merge_tile_results(\n",
    "        self, \n",
    "        tile_results: list, \n",
    "        tile_metadata: list, \n",
    "        analysis_id: str\n",
    "    ) -> dict:\n",
    "        \"\"\"íƒ€ì¼ ê²°ê³¼ ë³‘í•© ì‹¤í–‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        logger.info(f\"íƒ€ì¼ ê²°ê³¼ ë³‘í•© ì‹œì‘: {len(tile_results)}ê°œ íƒ€ì¼\")\n",
    "        \n",
    "        # 1. ì „ì—­ ì¢Œí‘œë¡œ ë³€í™˜\n",
    "        global_detections = self.convert_to_global_coords(tile_results, tile_metadata)\n",
    "        \n",
    "        # 2. ê²¹ì¹˜ëŠ” ê²€ì¶œ ë³‘í•©\n",
    "        merged_detections = self.merge_overlapping_detections(global_detections)\n",
    "        \n",
    "        # 3. ê²°ê³¼ í†µê³„ ê³„ì‚°\n",
    "        class_counts = {}\n",
    "        for detection in merged_detections:\n",
    "            class_name = detection['class_name']\n",
    "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "        \n",
    "        merge_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'analysis_id': analysis_id,\n",
    "            'detections': merged_detections,\n",
    "            'total_detections': len(merged_detections),\n",
    "            'class_counts': class_counts,\n",
    "            'tiles_processed': len(tile_results),\n",
    "            'merge_time': merge_time,\n",
    "            'iou_threshold': self.iou_threshold,\n",
    "            'confidence_threshold': self.confidence_threshold\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"ë³‘í•© ì™„ë£Œ: {len(merged_detections)}ê°œ ìµœì¢… ê²€ì¶œ, {merge_time:.2f}ì´ˆ\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ì „ì—­ ë³‘í•© ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "merge_engine = MergeEngine()\n",
    "\n",
    "print(\"âœ… POD5 ê²°ê³¼ ë³‘í•© ì—”ì§„ í†µí•© ì™„ë£Œ (v2.0 ì‹ ê·œ)\")\n",
    "print(f\"ğŸ”— IoU ì„ê³„ê°’: {merge_engine.iou_threshold}\")\n",
    "print(f\"ğŸ“Š ì‹ ë¢°ë„ ì„ê³„ê°’: {merge_engine.confidence_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pod6_export",
   "metadata": {},
   "source": [
    "### POD6: GPKG ë‚´ë³´ë‚´ê¸° ì—”ì§„ (v2.0 ì‹ ê·œ ì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pod6_export_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD6: GPKG ë‚´ë³´ë‚´ê¸° ì—”ì§„ (v2.0 ì‹ ê·œ)\n",
    "class GPKGExporter:\n",
    "    \"\"\"GPKG íŒŒì¼ ë‚´ë³´ë‚´ê¸° ì—”ì§„\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = None):\n",
    "        self.output_dir = Path(output_dir or settings.EXPORT_PATH)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def apply_privacy_protection(self, data: gpd.GeoDataFrame, privacy_config: dict = None) -> gpd.GeoDataFrame:\n",
    "        \"\"\"ê°œì¸ì •ë³´ ë³´í˜¸ ì ìš©\"\"\"\n",
    "        if not privacy_config or not privacy_config.get('mask_pnu', settings.PRIVACY_MASK):\n",
    "            return data\n",
    "        \n",
    "        # PNU ë§ˆìŠ¤í‚¹ (ë’¤ 4ìë¦¬)\n",
    "        if 'pnu' in data.columns:\n",
    "            data = data.copy()\n",
    "            data['pnu_masked'] = data['pnu'].apply(\n",
    "                lambda x: x[:15] + '****' if len(str(x)) == 19 else x\n",
    "            )\n",
    "            data.drop('pnu', axis=1, inplace=True)\n",
    "            data.rename(columns={'pnu_masked': 'pnu'}, inplace=True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def create_detection_layer(self, detections: list, image_metadata: dict) -> gpd.GeoDataFrame:\n",
    "        \"\"\"ê²€ì¶œ ê²°ê³¼ ë ˆì´ì–´ ìƒì„±\"\"\"\n",
    "        if not detections:\n",
    "            return gpd.GeoDataFrame()\n",
    "        \n",
    "        geometries = []\n",
    "        attributes = []\n",
    "        \n",
    "        for i, detection in enumerate(detections):\n",
    "            # ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í´ë¦¬ê³¤ìœ¼ë¡œ ë³€í™˜\n",
    "            bbox = detection['bbox']\n",
    "            polygon = box(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "            geometries.append(polygon)\n",
    "            \n",
    "            # ì†ì„± ë°ì´í„°\n",
    "            attributes.append({\n",
    "                'detection_id': i + 1,\n",
    "                'class_name': detection['class_name'],\n",
    "                'confidence': round(detection['confidence'], 3),\n",
    "                'area_px': int(detection['area']),\n",
    "                'bbox_x1': int(bbox[0]),\n",
    "                'bbox_y1': int(bbox[1]),\n",
    "                'bbox_x2': int(bbox[2]),\n",
    "                'bbox_y2': int(bbox[3]),\n",
    "                'source_tile': detection.get('source_tile', 'unknown')\n",
    "            })\n",
    "        \n",
    "        # GeoDataFrame ìƒì„±\n",
    "        gdf = gpd.GeoDataFrame(attributes, geometry=geometries)\n",
    "        \n",
    "        # ì¢Œí‘œê³„ ì„¤ì • (í”½ì…€ ì¢Œí‘œê³„ì—ì„œ ì§€ë¦¬ ì¢Œí‘œê³„ë¡œ ë³€í™˜ í•„ìš”)\n",
    "        # ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ EPSG:4326 ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ ì¢Œí‘œê³„ ë³€í™˜ í•„ìš”)\n",
    "        gdf.crs = \"EPSG:4326\"\n",
    "        \n",
    "        return gdf\n",
    "    \n",
    "    def create_statistics_layer(self, detections: list, analysis_metadata: dict) -> gpd.GeoDataFrame:\n",
    "        \"\"\"í†µê³„ ë ˆì´ì–´ ìƒì„±\"\"\"\n",
    "        # í´ë˜ìŠ¤ë³„ í†µê³„ ê³„ì‚°\n",
    "        class_stats = {}\n",
    "        for detection in detections:\n",
    "            class_name = detection['class_name']\n",
    "            if class_name not in class_stats:\n",
    "                class_stats[class_name] = {\n",
    "                    'count': 0,\n",
    "                    'total_area': 0,\n",
    "                    'avg_confidence': 0,\n",
    "                    'confidences': []\n",
    "                }\n",
    "            \n",
    "            class_stats[class_name]['count'] += 1\n",
    "            class_stats[class_name]['total_area'] += detection['area']\n",
    "            class_stats[class_name]['confidences'].append(detection['confidence'])\n",
    "        \n",
    "        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°\n",
    "        for class_name, stats in class_stats.items():\n",
    "            stats['avg_confidence'] = np.mean(stats['confidences'])\n",
    "        \n",
    "        # Point ì§€ì˜¤ë©”íŠ¸ë¦¬ë¡œ í†µê³„ ë°ì´í„° ìƒì„± (ì„ì˜ ì¢Œí‘œ)\n",
    "        geometries = []\n",
    "        attributes = []\n",
    "        \n",
    "        for i, (class_name, stats) in enumerate(class_stats.items()):\n",
    "            # ì„ì˜ì˜ ì  ì¢Œí‘œ (ì‹¤ì œë¡œëŠ” ë¶„ì„ ì˜ì—­ ì¤‘ì‹¬ì  ì‚¬ìš©)\n",
    "            point = Point(127.0 + i * 0.001, 37.0 + i * 0.001)\n",
    "            geometries.append(point)\n",
    "            \n",
    "            attributes.append({\n",
    "                'class_name': class_name,\n",
    "                'detection_count': stats['count'],\n",
    "                'total_area_px': int(stats['total_area']),\n",
    "                'avg_confidence': round(stats['avg_confidence'], 3),\n",
    "                'analysis_date': datetime.now().isoformat()[:10]\n",
    "            })\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(attributes, geometry=geometries)\n",
    "        gdf.crs = \"EPSG:4326\"\n",
    "        \n",
    "        return gdf\n",
    "    \n",
    "    def export_to_gpkg(\n",
    "        self, \n",
    "        detections: list, \n",
    "        analysis_id: str, \n",
    "        analysis_metadata: dict = None,\n",
    "        export_config: dict = None\n",
    "    ) -> dict:\n",
    "        \"\"\"GPKG íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # íŒŒì¼ëª… ìƒì„±\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        region_name = analysis_metadata.get('region_name', 'unknown') if analysis_metadata else 'unknown'\n",
    "        filename = f\"{region_name}_{timestamp}_analysis.gpkg\"\n",
    "        output_path = self.output_dir / filename\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"GPKG ë‚´ë³´ë‚´ê¸° ì‹œì‘: {output_path}\")\n",
    "            \n",
    "            # ê²€ì¶œ ê²°ê³¼ ë ˆì´ì–´ ìƒì„±\n",
    "            detection_layer = self.create_detection_layer(detections, analysis_metadata or {})\n",
    "            \n",
    "            if not detection_layer.empty:\n",
    "                # ê°œì¸ì •ë³´ ë³´í˜¸ ì ìš©\n",
    "                if export_config and export_config.get('apply_privacy', True):\n",
    "                    detection_layer = self.apply_privacy_protection(\n",
    "                        detection_layer, \n",
    "                        export_config.get('privacy_config', {})\n",
    "                    )\n",
    "                \n",
    "                # ì¶œë ¥ ì¢Œí‘œê³„ë¡œ ë³€í™˜\n",
    "                if detection_layer.crs != settings.OUTPUT_CRS:\n",
    "                    detection_layer = detection_layer.to_crs(settings.OUTPUT_CRS)\n",
    "                \n",
    "                # GPKGì— ê²€ì¶œ ë ˆì´ì–´ ì €ì¥\n",
    "                detection_layer.to_file(\n",
    "                    output_path, \n",
    "                    layer='detections', \n",
    "                    driver=\"GPKG\"\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"ê²€ì¶œ ë ˆì´ì–´ ì €ì¥ ì™„ë£Œ: {len(detection_layer)}ê°œ ê°ì²´\")\n",
    "            \n",
    "            # í†µê³„ ë ˆì´ì–´ ìƒì„± ë° ì €ì¥\n",
    "            if export_config and export_config.get('include_statistics', True):\n",
    "                stats_layer = self.create_statistics_layer(detections, analysis_metadata or {})\n",
    "                \n",
    "                if not stats_layer.empty:\n",
    "                    if stats_layer.crs != settings.OUTPUT_CRS:\n",
    "                        stats_layer = stats_layer.to_crs(settings.OUTPUT_CRS)\n",
    "                    \n",
    "                    stats_layer.to_file(\n",
    "                        output_path, \n",
    "                        layer='statistics', \n",
    "                        driver=\"GPKG\"\n",
    "                    )\n",
    "                    \n",
    "                    logger.info(f\"í†µê³„ ë ˆì´ì–´ ì €ì¥ ì™„ë£Œ: {len(stats_layer)}ê°œ í´ë˜ìŠ¤\")\n",
    "            \n",
    "            # ë©”íƒ€ë°ì´í„° ë ˆì´ì–´ ì¶”ê°€\n",
    "            if export_config and export_config.get('include_metadata', True):\n",
    "                metadata_dict = {\n",
    "                    'analysis_id': analysis_id,\n",
    "                    'export_date': datetime.now().isoformat(),\n",
    "                    'total_detections': len(detections),\n",
    "                    'output_crs': settings.OUTPUT_CRS,\n",
    "                    'privacy_applied': export_config.get('apply_privacy', True)\n",
    "                }\n",
    "                \n",
    "                if analysis_metadata:\n",
    "                    metadata_dict.update(analysis_metadata)\n",
    "                \n",
    "                # ë©”íƒ€ë°ì´í„°ë¥¼ Pointë¡œ ì €ì¥\n",
    "                metadata_gdf = gpd.GeoDataFrame(\n",
    "                    [metadata_dict],\n",
    "                    geometry=[Point(0, 0)],\n",
    "                    crs=\"EPSG:4326\"\n",
    "                )\n",
    "                \n",
    "                metadata_gdf.to_file(\n",
    "                    output_path, \n",
    "                    layer='metadata', \n",
    "                    driver=\"GPKG\"\n",
    "                )\n",
    "            \n",
    "            export_time = time.time() - start_time\n",
    "            file_size = output_path.stat().st_size\n",
    "            \n",
    "            result = {\n",
    "                'export_id': str(uuid4()),\n",
    "                'file_path': str(output_path),\n",
    "                'filename': filename,\n",
    "                'file_size': file_size,\n",
    "                'layer_count': 2 + (1 if export_config and export_config.get('include_metadata', True) else 0),\n",
    "                'total_detections': len(detections),\n",
    "                'export_time': export_time,\n",
    "                'crs': settings.OUTPUT_CRS\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"GPKG ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename} ({file_size} bytes, {export_time:.2f}ì´ˆ)\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"GPKG ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "\n",
    "# ì „ì—­ GPKG ë‚´ë³´ë‚´ê¸° ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "gpkg_exporter = GPKGExporter()\n",
    "\n",
    "print(\"âœ… POD6 GPKG ë‚´ë³´ë‚´ê¸° ì—”ì§„ í†µí•© ì™„ë£Œ (v2.0 ì‹ ê·œ)\")\n",
    "print(f\"ğŸ“¦ ì¶œë ¥ ë””ë ‰í† ë¦¬: {gpkg_exporter.output_dir}\")\n",
    "print(f\"ğŸ—ºï¸ ì¶œë ¥ ì¢Œí‘œê³„: {settings.OUTPUT_CRS}\")\n",
    "print(f\"ğŸ”’ ê°œì¸ì •ë³´ ë³´í˜¸: {settings.PRIVACY_MASK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api_schemas",
   "metadata": {},
   "source": [
    "## ğŸ“‹ 6. API ìŠ¤í‚¤ë§ˆ ì •ì˜ (v2.0 í™•ì¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api_schemas_definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API ì‘ë‹µ ìŠ¤í‚¤ë§ˆ (v2.0 í™•ì¥)\n",
    "class APIResponse(BaseModel):\n",
    "    \"\"\"ê¸°ë³¸ API ì‘ë‹µ\"\"\"\n",
    "    success: bool = True\n",
    "    message: str = \"\"\n",
    "    data: Optional[Any] = None\n",
    "    error: Optional[str] = None\n",
    "    version: str = \"2.0.0\"  # v2.0 ì¶”ê°€\n",
    "\n",
    "# ì´ë¯¸ì§€ ê´€ë ¨ ìŠ¤í‚¤ë§ˆ\n",
    "class ImageUploadRequest(BaseModel):\n",
    "    \"\"\"ì´ë¯¸ì§€ ì—…ë¡œë“œ ìš”ì²­\"\"\"\n",
    "    description: Optional[str] = None\n",
    "    region_name: Optional[str] = None\n",
    "    tags: List[str] = []\n",
    "\n",
    "class ImageResponse(BaseModel):\n",
    "    \"\"\"ì´ë¯¸ì§€ ì‘ë‹µ\"\"\"\n",
    "    id: str\n",
    "    filename: str\n",
    "    file_path: str\n",
    "    file_size: int\n",
    "    status: str\n",
    "    format: str\n",
    "    description: Optional[str] = None\n",
    "    region_name: Optional[str] = None\n",
    "    tags: List[str] = []\n",
    "    metadata: Optional[dict] = None\n",
    "    uploaded_at: datetime\n",
    "    updated_at: datetime\n",
    "\n",
    "# ë¶„ì„ ê´€ë ¨ ìŠ¤í‚¤ë§ˆ (v2.0 í™•ì¥)\n",
    "class AnalysisRequest(BaseModel):\n",
    "    \"\"\"ë¶„ì„ ìš”ì²­ v2.0\"\"\"\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "    analysis_type: str  # crop, facility, landuse\n",
    "    confidence_threshold: float = Field(0.5, ge=0.1, le=1.0)\n",
    "    use_tiling: bool = False  # v2.0 ì¶”ê°€\n",
    "    tile_size: Optional[int] = Field(None, ge=256, le=2048)  # v2.0 ì¶”ê°€\n",
    "    tile_overlap: Optional[float] = Field(None, ge=0.0, le=0.5)  # v2.0 ì¶”ê°€\n",
    "    shapefile_path: Optional[str] = None  # í¬ë¡œí•‘ìš© Shapefile\n",
    "    merge_results: bool = True  # v2.0 ì¶”ê°€\n",
    "\n",
    "class AnalysisResponse(BaseModel):\n",
    "    \"\"\"ë¶„ì„ ì‘ë‹µ v2.0\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "    status: str\n",
    "    analysis_type: str\n",
    "    confidence_threshold: float\n",
    "    use_tiling: bool = False  # v2.0 ì¶”ê°€\n",
    "    total_detections: int = 0\n",
    "    total_tiles: int = 0  # v2.0 ì¶”ê°€\n",
    "    processing_time: Optional[float] = None\n",
    "    error_message: Optional[str] = None\n",
    "    created_at: datetime\n",
    "    started_at: Optional[datetime] = None\n",
    "    completed_at: Optional[datetime] = None\n",
    "\n",
    "# í¬ë¡­ ê²°ê³¼ ìŠ¤í‚¤ë§ˆ\n",
    "class CropResultResponse(BaseModel):\n",
    "    \"\"\"í¬ë¡­ ê²°ê³¼ ì‘ë‹µ\"\"\"\n",
    "    id: str\n",
    "    pnu: str\n",
    "    crop_path: str\n",
    "    crop_size: int\n",
    "    geometry_type: str\n",
    "    area: float\n",
    "    detected_class: Optional[str] = None\n",
    "    confidence: Optional[float] = None\n",
    "    detection_count: int = 0\n",
    "    merged_from_tiles: bool = False  # v2.0 ì¶”ê°€\n",
    "    created_at: datetime\n",
    "\n",
    "# ë‚´ë³´ë‚´ê¸° ê´€ë ¨ ìŠ¤í‚¤ë§ˆ (v2.0 ì‹ ê·œ)\n",
    "class ExportRequest(BaseModel):\n",
    "    \"\"\"ë‚´ë³´ë‚´ê¸° ìš”ì²­\"\"\"\n",
    "    name: str\n",
    "    export_type: str = \"gpkg\"  # gpkg, geojson, csv, shapefile\n",
    "    include_statistics: bool = True\n",
    "    include_metadata: bool = True\n",
    "    apply_privacy: bool = True\n",
    "    filter_confidence: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "    filter_classes: Optional[List[str]] = None\n",
    "\n",
    "class ExportResponse(BaseModel):\n",
    "    \"\"\"ë‚´ë³´ë‚´ê¸° ì‘ë‹µ\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    export_type: str\n",
    "    status: str\n",
    "    file_path: Optional[str] = None\n",
    "    filename: Optional[str] = None\n",
    "    file_size: Optional[int] = None\n",
    "    layer_count: int = 0\n",
    "    download_count: int = 0\n",
    "    created_at: datetime\n",
    "    completed_at: Optional[datetime] = None\n",
    "    expires_at: Optional[datetime] = None\n",
    "\n",
    "# í†µê³„ ê´€ë ¨ ìŠ¤í‚¤ë§ˆ (v2.0 ì‹ ê·œ)\n",
    "class StatisticsResponse(BaseModel):\n",
    "    \"\"\"í†µê³„ ì‘ë‹µ\"\"\"\n",
    "    total_images: int\n",
    "    total_analyses: int\n",
    "    total_detections: int\n",
    "    total_exports: int\n",
    "    analysis_by_type: Dict[str, int]\n",
    "    detections_by_class: Dict[str, int]\n",
    "    avg_processing_time: float\n",
    "    last_updated: datetime\n",
    "\n",
    "print(\"âœ… API ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ (v2.0 í™•ì¥)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fastapi_app",
   "metadata": {},
   "source": [
    "## ğŸš€ 7. FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ (v2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fastapi_application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± (v2.0)\n",
    "app = FastAPI(\n",
    "    title=settings.PROJECT_NAME,\n",
    "    description=\"AI ê¸°ë°˜ ë†ì—… ì˜ìƒ ë¶„ì„ í”Œë«í¼ API v2.0 - ì™„ì „ í†µí•© ë²„ì „\",\n",
    "    version=settings.VERSION,\n",
    "    docs_url=\"/api/docs\",\n",
    "    redoc_url=\"/api/redoc\"\n",
    ")\n",
    "\n",
    "# CORS ë¯¸ë“¤ì›¨ì–´\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# í—¬ìŠ¤ ì²´í¬\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ v2.0\"\"\"\n",
    "    return APIResponse(\n",
    "        success=True,\n",
    "        message=\"ì„œë¹„ìŠ¤ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤ (v2.0)\",\n",
    "        data={\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"version\": settings.VERSION,\n",
    "            \"environment\": settings.ENVIRONMENT,\n",
    "            \"features\": {\n",
    "                \"tiling\": True,\n",
    "                \"merging\": True,\n",
    "                \"gpkg_export\": True,\n",
    "                \"statistics\": True\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"âœ… FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ì™„ë£Œ (v2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "images_api",
   "metadata": {},
   "source": [
    "## ğŸ“¸ 8. Images API êµ¬í˜„ (v1.0ê³¼ ë™ì¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "images_api_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images API ì—”ë“œí¬ì¸íŠ¸ (v1.0ê³¼ ë™ì¼)\n",
    "@app.post(\"/api/v1/images/upload\", response_model=APIResponse)\n",
    "async def upload_image(\n",
    "    file: UploadFile = File(...),\n",
    "    description: str = None,\n",
    "    region_name: str = None,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ì´ë¯¸ì§€ ì—…ë¡œë“œ\"\"\"\n",
    "    try:\n",
    "        # íŒŒì¼ í™•ì¥ì ê²€ì¦\n",
    "        allowed_extensions = ['.tif', '.tiff', '.jp2']\n",
    "        file_extension = Path(file.filename).suffix.lower()\n",
    "        if file_extension not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}\"\n",
    "            )\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        file_path, file_size = data_registry.save_file(file)\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "        try:\n",
    "            metadata = data_registry.extract_image_metadata(Path(file_path))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "            metadata = {}\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ê°€ì ¸ì˜¤ê¸°\n",
    "        test_user = db.query(User).filter(User.username == \"testuser\").first()\n",
    "        if not test_user:\n",
    "            raise HTTPException(status_code=404, detail=\"í…ŒìŠ¤íŠ¸ ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        # ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ì§€ ì •ë³´ ì €ì¥\n",
    "        db_image = Image(\n",
    "            filename=file.filename,\n",
    "            file_path=file_path,\n",
    "            file_size=file_size,\n",
    "            format=file_extension.replace('.', ''),\n",
    "            status=\"ready\",\n",
    "            description=description,\n",
    "            region_name=region_name,\n",
    "            owner_id=test_user.id,\n",
    "            crs=metadata.get('crs'),\n",
    "            bounds=metadata.get('bounds'),\n",
    "            resolution=metadata.get('resolution'),\n",
    "            width=metadata.get('width'),\n",
    "            height=metadata.get('height'),\n",
    "            bands=metadata.get('bands')\n",
    "        )\n",
    "        \n",
    "        db.add(db_image)\n",
    "        db.commit()\n",
    "        db.refresh(db_image)\n",
    "        \n",
    "        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±\n",
    "        response_data = ImageResponse(\n",
    "            id=db_image.id,\n",
    "            filename=db_image.filename,\n",
    "            file_path=db_image.file_path,\n",
    "            file_size=db_image.file_size,\n",
    "            status=db_image.status,\n",
    "            format=db_image.format,\n",
    "            description=db_image.description,\n",
    "            region_name=db_image.region_name,\n",
    "            tags=[],\n",
    "            metadata=metadata,\n",
    "            uploaded_at=db_image.uploaded_at,\n",
    "            updated_at=db_image.updated_at\n",
    "        )\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=\"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ\",\n",
    "            data=response_data.dict()\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.get(\"/api/v1/images\", response_model=APIResponse)\n",
    "async def list_images(db: Session = Depends(get_db)):\n",
    "    \"\"\"ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ\"\"\"\n",
    "    try:\n",
    "        images = db.query(Image).all()\n",
    "        \n",
    "        image_list = []\n",
    "        for img in images:\n",
    "            image_data = ImageResponse(\n",
    "                id=img.id,\n",
    "                filename=img.filename,\n",
    "                file_path=img.file_path,\n",
    "                file_size=img.file_size,\n",
    "                status=img.status,\n",
    "                format=img.format,\n",
    "                description=img.description,\n",
    "                region_name=img.region_name,\n",
    "                tags=img.tags or [],\n",
    "                uploaded_at=img.uploaded_at,\n",
    "                updated_at=img.updated_at\n",
    "            )\n",
    "            image_list.append(image_data.dict())\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=f\"{len(image_list)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤\",\n",
    "            data={\"images\": image_list, \"total\": len(image_list)}\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.get(\"/api/v1/images/{image_id}\", response_model=APIResponse)\n",
    "async def get_image(image_id: str, db: Session = Depends(get_db)):\n",
    "    \"\"\"ì´ë¯¸ì§€ ìƒì„¸ ì¡°íšŒ\"\"\"\n",
    "    try:\n",
    "        image = db.query(Image).filter(Image.id == image_id).first()\n",
    "        if not image:\n",
    "            raise HTTPException(status_code=404, detail=\"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        image_data = ImageResponse(\n",
    "            id=image.id,\n",
    "            filename=image.filename,\n",
    "            file_path=image.file_path,\n",
    "            file_size=image.file_size,\n",
    "            status=image.status,\n",
    "            format=image.format,\n",
    "            description=image.description,\n",
    "            region_name=image.region_name,\n",
    "            tags=image.tags or [],\n",
    "            metadata={\n",
    "                \"crs\": image.crs,\n",
    "                \"bounds\": image.bounds,\n",
    "                \"resolution\": image.resolution,\n",
    "                \"width\": image.width,\n",
    "                \"height\": image.height,\n",
    "                \"bands\": image.bands\n",
    "            },\n",
    "            uploaded_at=image.uploaded_at,\n",
    "            updated_at=image.updated_at\n",
    "        )\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=\"ì´ë¯¸ì§€ ì¡°íšŒ ì™„ë£Œ\",\n",
    "            data=image_data.dict()\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "print(\"âœ… Images API êµ¬í˜„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pf0zqd1ekq",
   "source": "## ğŸ¯ 9. Analysis API v2.0 (íƒ€ì¼ë§ ì§€ì›)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x5g62msq4ig",
   "source": "# Analysis API v2.0 (íƒ€ì¼ë§ ì§€ì›)\n@app.post(\"/api/v1/analyses\", response_model=APIResponse)\nasync def create_analysis(\n    image_id: str,\n    request: AnalysisRequest,\n    background_tasks: BackgroundTasks,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ë¶„ì„ ì‘ì—… ìƒì„± v2.0 (íƒ€ì¼ë§ ì§€ì›)\"\"\"\n    try:\n        # ì´ë¯¸ì§€ ì¡´ì¬ í™•ì¸\n        image = db.query(Image).filter(Image.id == image_id).first()\n        if not image:\n            raise HTTPException(status_code=404, detail=\"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ê°€ì ¸ì˜¤ê¸°\n        test_user = db.query(User).filter(User.username == \"testuser\").first()\n        if not test_user:\n            raise HTTPException(status_code=404, detail=\"ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # ë¶„ì„ ì‘ì—… ìƒì„±\n        analysis = Analysis(\n            name=request.name,\n            description=request.description,\n            status=\"pending\",\n            analysis_type=request.analysis_type,\n            confidence_threshold=request.confidence_threshold,\n            use_tiling=request.use_tiling,\n            config={\n                \"tile_size\": request.tile_size or settings.TILE_SIZE,\n                \"tile_overlap\": request.tile_overlap or settings.TILE_OVERLAP,\n                \"shapefile_path\": request.shapefile_path,\n                \"merge_results\": request.merge_results\n            },\n            image_id=image_id,\n            owner_id=test_user.id\n        )\n        \n        db.add(analysis)\n        db.commit()\n        db.refresh(analysis)\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰\n        background_tasks.add_task(\n            run_analysis_v2,\n            analysis.id,\n            image.file_path,\n            request\n        )\n        \n        response_data = AnalysisResponse(\n            id=analysis.id,\n            name=analysis.name,\n            description=analysis.description,\n            status=analysis.status,\n            analysis_type=analysis.analysis_type,\n            confidence_threshold=analysis.confidence_threshold,\n            use_tiling=analysis.use_tiling,\n            created_at=analysis.created_at\n        )\n        \n        return APIResponse(\n            success=True,\n            message=\"ë¶„ì„ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ (v2.0)\",\n            data=response_data.dict()\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n\n\n# ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜ v2.0\ndef run_analysis_v2(analysis_id: str, image_path: str, request: AnalysisRequest):\n    \"\"\"ë¶„ì„ ì‹¤í–‰ (v2.0 - íƒ€ì¼ë§ ì§€ì›)\"\"\"\n    db = SessionLocal()\n    \n    try:\n        # ë¶„ì„ ì‘ì—… ì¡°íšŒ\n        analysis = db.query(Analysis).filter(Analysis.id == analysis_id).first()\n        if not analysis:\n            logger.error(f\"ë¶„ì„ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {analysis_id}\")\n            return\n        \n        # ìƒíƒœ ì—…ë°ì´íŠ¸\n        analysis.status = \"running\"\n        analysis.started_at = datetime.utcnow()\n        db.commit()\n        \n        logger.info(f\"ë¶„ì„ ì‹œì‘: {analysis_id} (v2.0)\")\n        \n        if request.use_tiling:\n            # íƒ€ì¼ë§ ê¸°ë°˜ ë¶„ì„\n            result = run_tiling_analysis(analysis_id, image_path, request, db)\n        else:\n            # ì¼ë°˜ ë¶„ì„ (í¬ë¡œí•‘ ê¸°ë°˜)\n            result = run_standard_analysis(analysis_id, image_path, request, db)\n        \n        # ë¶„ì„ ì™„ë£Œ ì²˜ë¦¬\n        analysis.status = \"completed\"\n        analysis.completed_at = datetime.utcnow()\n        analysis.total_detections = result.get('total_detections', 0)\n        analysis.total_tiles = result.get('total_tiles', 0)\n        analysis.processing_time = result.get('processing_time', 0)\n        analysis.result_data = result\n        \n        db.commit()\n        \n        logger.info(f\"ë¶„ì„ ì™„ë£Œ: {analysis_id}\")\n        \n    except Exception as e:\n        logger.error(f\"ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        \n        # ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸\n        analysis.status = \"error\"\n        analysis.error_message = str(e)\n        analysis.completed_at = datetime.utcnow()\n        db.commit()\n        \n    finally:\n        db.close()\n\n\ndef run_tiling_analysis(analysis_id: str, image_path: str, request: AnalysisRequest, db: Session) -> dict:\n    \"\"\"íƒ€ì¼ë§ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰\"\"\"\n    start_time = time.time()\n    \n    # 1. ì´ë¯¸ì§€ íƒ€ì¼ë§\n    logger.info(\"íƒ€ì¼ë§ ì‹œì‘\")\n    tiles = tiling_engine.generate_tiles(image_path, analysis_id)\n    \n    # 2. íƒ€ì¼ ì •ë³´ DB ì €ì¥\n    for tile_info in tiles:\n        tile = Tile(\n            tile_path=tile_info['tile_path'],\n            tile_size=tile_info['tile_size'],\n            row=tile_info['row'],\n            col=tile_info['col'],\n            x_offset=tile_info['x_offset'],\n            y_offset=tile_info['y_offset'],\n            bounds=tile_info['bounds'],\n            pixel_bounds=tile_info['pixel_bounds'],\n            image_id=db.query(Analysis).filter(Analysis.id == analysis_id).first().image_id,\n            analysis_id=analysis_id\n        )\n        db.add(tile)\n    \n    db.commit()\n    \n    # 3. AI ì¶”ë¡  (ë°°ì¹˜ ì²˜ë¦¬)\n    logger.info(f\"AI ì¶”ë¡  ì‹œì‘: {len(tiles)}ê°œ íƒ€ì¼\")\n    tile_paths = [tile['tile_path'] for tile in tiles]\n    tile_results = ai_engine.predict_batch(\n        tile_paths, \n        request.analysis_type, \n        request.confidence_threshold\n    )\n    \n    # 4. ê²°ê³¼ ë³‘í•©\n    logger.info(\"ê²°ê³¼ ë³‘í•© ì‹œì‘\")\n    merged_result = merge_engine.merge_tile_results(\n        tile_results,\n        tiles,\n        analysis_id\n    )\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        'total_detections': merged_result['total_detections'],\n        'total_tiles': len(tiles),\n        'processing_time': processing_time,\n        'class_counts': merged_result['class_counts'],\n        'merge_stats': {\n            'iou_threshold': merged_result['iou_threshold'],\n            'confidence_threshold': merged_result['confidence_threshold']\n        }\n    }\n\n\ndef run_standard_analysis(analysis_id: str, image_path: str, request: AnalysisRequest, db: Session) -> dict:\n    \"\"\"í‘œì¤€ ë¶„ì„ ì‹¤í–‰ (í¬ë¡œí•‘ ê¸°ë°˜)\"\"\"\n    start_time = time.time()\n    \n    if not request.shapefile_path or not Path(request.shapefile_path).exists():\n        raise ValueError(\"Shapefile ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤\")\n    \n    # 1. í¬ë¡œí•‘\n    logger.info(\"í¬ë¡œí•‘ ì‹œì‘\")\n    crop_results = cropping_engine.crop_by_shapes(image_path, request.shapefile_path)\n    \n    # 2. AI ì¶”ë¡ \n    logger.info(f\"AI ì¶”ë¡  ì‹œì‘: {len(crop_results)}ê°œ í¬ë¡­\")\n    crop_paths = [result['crop_path'] for result in crop_results]\n    ai_results = ai_engine.predict_batch(\n        crop_paths,\n        request.analysis_type,\n        request.confidence_threshold\n    )\n    \n    # 3. ê²°ê³¼ DB ì €ì¥\n    total_detections = 0\n    for crop_result, ai_result in zip(crop_results, ai_results):\n        if 'error' in ai_result:\n            continue\n            \n        detection_count = ai_result.get('detection_count', 0)\n        detected_class = None\n        confidence = 0.0\n        \n        if ai_result.get('detections'):\n            # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ ê²€ì¶œ ì‚¬ìš©\n            best_detection = max(ai_result['detections'], key=lambda x: x['confidence'])\n            detected_class = best_detection['class_name']\n            confidence = best_detection['confidence']\n        \n        crop_db = CropResult(\n            pnu=crop_result['pnu'],\n            crop_path=crop_result['crop_path'],\n            crop_size=crop_result['crop_size'],\n            geometry_type=crop_result['geometry_type'],\n            area=crop_result['area'],\n            bounds=crop_result['bounds'],\n            detected_class=detected_class,\n            confidence=confidence,\n            detection_count=detection_count,\n            detection_data=ai_result,\n            image_id=db.query(Analysis).filter(Analysis.id == analysis_id).first().image_id,\n            analysis_id=analysis_id\n        )\n        \n        db.add(crop_db)\n        total_detections += detection_count\n    \n    db.commit()\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        'total_detections': total_detections,\n        'total_tiles': 0,\n        'processing_time': processing_time,\n        'total_crops': len(crop_results)\n    }\n\n\n@app.get(\"/api/v1/analyses\", response_model=APIResponse)\nasync def list_analyses(db: Session = Depends(get_db)):\n    \"\"\"ë¶„ì„ ëª©ë¡ ì¡°íšŒ\"\"\"\n    try:\n        analyses = db.query(Analysis).all()\n        \n        analysis_list = []\n        for analysis in analyses:\n            analysis_data = AnalysisResponse(\n                id=analysis.id,\n                name=analysis.name,\n                description=analysis.description,\n                status=analysis.status,\n                analysis_type=analysis.analysis_type,\n                confidence_threshold=analysis.confidence_threshold,\n                use_tiling=analysis.use_tiling,\n                total_detections=analysis.total_detections,\n                total_tiles=analysis.total_tiles,\n                processing_time=analysis.processing_time,\n                error_message=analysis.error_message,\n                created_at=analysis.created_at,\n                started_at=analysis.started_at,\n                completed_at=analysis.completed_at\n            )\n            analysis_list.append(analysis_data.dict())\n        \n        return APIResponse(\n            success=True,\n            message=f\"{len(analysis_list)}ê°œì˜ ë¶„ì„ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤\",\n            data={\"analyses\": analysis_list, \"total\": len(analysis_list)}\n        )\n        \n    except Exception as e:\n        logger.error(f\"ë¶„ì„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë¶„ì„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@app.get(\"/api/v1/analyses/{analysis_id}\", response_model=APIResponse)\nasync def get_analysis(analysis_id: str, db: Session = Depends(get_db)):\n    \"\"\"ë¶„ì„ ìƒì„¸ ì¡°íšŒ\"\"\"\n    try:\n        analysis = db.query(Analysis).filter(Analysis.id == analysis_id).first()\n        if not analysis:\n            raise HTTPException(status_code=404, detail=\"ë¶„ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        analysis_data = AnalysisResponse(\n            id=analysis.id,\n            name=analysis.name,\n            description=analysis.description,\n            status=analysis.status,\n            analysis_type=analysis.analysis_type,\n            confidence_threshold=analysis.confidence_threshold,\n            use_tiling=analysis.use_tiling,\n            total_detections=analysis.total_detections,\n            total_tiles=analysis.total_tiles,\n            processing_time=analysis.processing_time,\n            error_message=analysis.error_message,\n            created_at=analysis.created_at,\n            started_at=analysis.started_at,\n            completed_at=analysis.completed_at\n        )\n        \n        return APIResponse(\n            success=True,\n            message=\"ë¶„ì„ ì¡°íšŒ ì™„ë£Œ\",\n            data=analysis_data.dict()\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"ë¶„ì„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë¶„ì„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\nprint(\"âœ… Analysis API v2.0 êµ¬í˜„ ì™„ë£Œ (íƒ€ì¼ë§ ì§€ì›)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2v8ulu01yu7",
   "source": "## ğŸŒ± 10. Crops API v2.0",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vxspfcq8v4",
   "source": "# Crops API v2.0\n@app.get(\"/api/v1/analyses/{analysis_id}/crops\", response_model=APIResponse)\nasync def get_analysis_crops(analysis_id: str, db: Session = Depends(get_db)):\n    \"\"\"ë¶„ì„ì˜ í¬ë¡­ ê²°ê³¼ ì¡°íšŒ\"\"\"\n    try:\n        # ë¶„ì„ ì¡´ì¬ í™•ì¸\n        analysis = db.query(Analysis).filter(Analysis.id == analysis_id).first()\n        if not analysis:\n            raise HTTPException(status_code=404, detail=\"ë¶„ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # í¬ë¡­ ê²°ê³¼ ì¡°íšŒ\n        crops = db.query(CropResult).filter(CropResult.analysis_id == analysis_id).all()\n        \n        crop_list = []\n        for crop in crops:\n            crop_data = CropResultResponse(\n                id=crop.id,\n                pnu=crop.pnu,\n                crop_path=crop.crop_path,\n                crop_size=crop.crop_size,\n                geometry_type=crop.geometry_type,\n                area=crop.area,\n                detected_class=crop.detected_class,\n                confidence=crop.confidence,\n                detection_count=crop.detection_count,\n                merged_from_tiles=crop.merged_from_tiles,\n                created_at=crop.created_at\n            )\n            crop_list.append(crop_data.dict())\n        \n        return APIResponse(\n            success=True,\n            message=f\"{len(crop_list)}ê°œì˜ í¬ë¡­ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤\",\n            data={\"crops\": crop_list, \"total\": len(crop_list)}\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"í¬ë¡­ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"í¬ë¡­ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@app.get(\"/api/v1/crops/{crop_id}\", response_model=APIResponse)\nasync def get_crop_detail(crop_id: str, db: Session = Depends(get_db)):\n    \"\"\"í¬ë¡­ ìƒì„¸ ì¡°íšŒ\"\"\"\n    try:\n        crop = db.query(CropResult).filter(CropResult.id == crop_id).first()\n        if not crop:\n            raise HTTPException(status_code=404, detail=\"í¬ë¡­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        crop_data = CropResultResponse(\n            id=crop.id,\n            pnu=crop.pnu,\n            crop_path=crop.crop_path,\n            crop_size=crop.crop_size,\n            geometry_type=crop.geometry_type,\n            area=crop.area,\n            detected_class=crop.detected_class,\n            confidence=crop.confidence,\n            detection_count=crop.detection_count,\n            merged_from_tiles=crop.merged_from_tiles,\n            created_at=crop.created_at\n        )\n        \n        return APIResponse(\n            success=True,\n            message=\"í¬ë¡­ ì¡°íšŒ ì™„ë£Œ\",\n            data={\n                \"crop\": crop_data.dict(),\n                \"detection_data\": crop.detection_data\n            }\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"í¬ë¡­ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"í¬ë¡­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@app.get(\"/api/v1/crops/{crop_id}/download\")\nasync def download_crop_image(crop_id: str, db: Session = Depends(get_db)):\n    \"\"\"í¬ë¡­ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\"\"\"\n    try:\n        crop = db.query(CropResult).filter(CropResult.id == crop_id).first()\n        if not crop:\n            raise HTTPException(status_code=404, detail=\"í¬ë¡­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        crop_path = Path(crop.crop_path)\n        if not crop_path.exists():\n            raise HTTPException(status_code=404, detail=\"í¬ë¡­ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        return FileResponse(\n            path=crop_path,\n            filename=f\"crop_{crop.pnu}.tif\",\n            media_type=\"image/tiff\"\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"í¬ë¡­ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"í¬ë¡­ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n\nprint(\"âœ… Crops API v2.0 êµ¬í˜„ ì™„ë£Œ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "g90c4e2mz4c",
   "source": "## ğŸ“¦ 11. Exports API v2.0 (GPKG ë‚´ë³´ë‚´ê¸°)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9nzaddcm2hh",
   "source": "# Exports API v2.0 (GPKG ë‚´ë³´ë‚´ê¸°)\n@app.post(\"/api/v1/analyses/{analysis_id}/exports\", response_model=APIResponse)\nasync def create_export(\n    analysis_id: str,\n    request: ExportRequest,\n    background_tasks: BackgroundTasks,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ìƒì„±\"\"\"\n    try:\n        # ë¶„ì„ ì¡´ì¬ í™•ì¸\n        analysis = db.query(Analysis).filter(Analysis.id == analysis_id).first()\n        if not analysis:\n            raise HTTPException(status_code=404, detail=\"ë¶„ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if analysis.status != \"completed\":\n            raise HTTPException(status_code=400, detail=\"ì™„ë£Œëœ ë¶„ì„ë§Œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n        \n        # ë‚´ë³´ë‚´ê¸° ì‘ì—… ìƒì„±\n        export = Export(\n            name=request.name,\n            export_type=request.export_type,\n            status=\"pending\",\n            include_geometry=True,\n            include_metadata=request.include_metadata,\n            privacy_applied=request.apply_privacy,\n            filter_config={\n                \"filter_confidence\": request.filter_confidence,\n                \"filter_classes\": request.filter_classes\n            },\n            analysis_id=analysis_id,\n            expires_at=datetime.utcnow() + timedelta(days=7)  # 7ì¼ í›„ ë§Œë£Œ\n        )\n        \n        db.add(export)\n        db.commit()\n        db.refresh(export)\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë‚´ë³´ë‚´ê¸° ì‹¤í–‰\n        background_tasks.add_task(\n            run_export_task,\n            export.id,\n            analysis_id,\n            request\n        )\n        \n        response_data = ExportResponse(\n            id=export.id,\n            name=export.name,\n            export_type=export.export_type,\n            status=export.status,\n            created_at=export.created_at,\n            expires_at=export.expires_at\n        )\n        \n        return APIResponse(\n            success=True,\n            message=\"ë‚´ë³´ë‚´ê¸° ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            data=response_data.dict()\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"ë‚´ë³´ë‚´ê¸° ìƒì„± ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë‚´ë³´ë‚´ê¸° ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n\n\ndef run_export_task(export_id: str, analysis_id: str, request: ExportRequest):\n    \"\"\"ë‚´ë³´ë‚´ê¸° ì‘ì—… ì‹¤í–‰\"\"\"\n    db = SessionLocal()\n    \n    try:\n        # ë‚´ë³´ë‚´ê¸° ì‘ì—… ì¡°íšŒ\n        export = db.query(Export).filter(Export.id == export_id).first()\n        if not export:\n            logger.error(f\"ë‚´ë³´ë‚´ê¸° ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {export_id}\")\n            return\n        \n        # ìƒíƒœ ì—…ë°ì´íŠ¸\n        export.status = \"processing\"\n        db.commit()\n        \n        logger.info(f\"ë‚´ë³´ë‚´ê¸° ì‹œì‘: {export_id}\")\n        \n        # ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘\n        analysis = db.query(Analysis).filter(Analysis.id == analysis_id).first()\n        \n        if analysis.use_tiling:\n            # íƒ€ì¼ë§ ê¸°ë°˜ ê²°ê³¼ ìˆ˜ì§‘\n            detections = collect_tiling_results(analysis_id, db)\n        else:\n            # í¬ë¡œí•‘ ê¸°ë°˜ ê²°ê³¼ ìˆ˜ì§‘\n            detections = collect_cropping_results(analysis_id, db)\n        \n        # í•„í„° ì ìš©\n        if request.filter_confidence:\n            detections = [\n                d for d in detections \n                if d.get('confidence', 0) >= request.filter_confidence\n            ]\n        \n        if request.filter_classes:\n            detections = [\n                d for d in detections \n                if d.get('class_name') in request.filter_classes\n            ]\n        \n        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„\n        image = db.query(Image).filter(Image.id == analysis.image_id).first()\n        analysis_metadata = {\n            'analysis_id': analysis_id,\n            'analysis_name': analysis.name,\n            'analysis_type': analysis.analysis_type,\n            'image_filename': image.filename if image else 'unknown',\n            'region_name': image.region_name if image else 'unknown',\n            'processing_time': analysis.processing_time\n        }\n        \n        # ë‚´ë³´ë‚´ê¸° ì„¤ì •\n        export_config = {\n            'include_statistics': request.include_statistics,\n            'include_metadata': request.include_metadata,\n            'apply_privacy': request.apply_privacy,\n            'privacy_config': {\n                'mask_pnu': True\n            }\n        }\n        \n        # GPKG ë‚´ë³´ë‚´ê¸° ì‹¤í–‰\n        if request.export_type == \"gpkg\":\n            result = gpkg_exporter.export_to_gpkg(\n                detections,\n                analysis_id,\n                analysis_metadata,\n                export_config\n            )\n        else:\n            raise ValueError(f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë‚´ë³´ë‚´ê¸° í˜•ì‹: {request.export_type}\")\n        \n        # ë‚´ë³´ë‚´ê¸° ì™„ë£Œ ì²˜ë¦¬\n        export.status = \"completed\"\n        export.file_path = result['file_path']\n        export.file_size = result['file_size']\n        export.layer_count = result['layer_count']\n        export.completed_at = datetime.utcnow()\n        \n        db.commit()\n        \n        logger.info(f\"ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {export_id}\")\n        \n    except Exception as e:\n        logger.error(f\"ë‚´ë³´ë‚´ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        \n        # ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸\n        export.status = \"error\"\n        export.error_message = str(e)\n        export.completed_at = datetime.utcnow()\n        db.commit()\n        \n    finally:\n        db.close()\n\n\ndef collect_tiling_results(analysis_id: str, db: Session) -> list:\n    \"\"\"íƒ€ì¼ë§ ê²°ê³¼ ìˆ˜ì§‘\"\"\"\n    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” analysis.result_dataì—ì„œ ë³‘í•©ëœ ê²€ì¶œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´\n    analysis = db.query(Analysis).filter(Analysis.id == analysis_id).first()\n    if analysis and analysis.result_data:\n        return analysis.result_data.get('detections', [])\n    return []\n\n\ndef collect_cropping_results(analysis_id: str, db: Session) -> list:\n    \"\"\"í¬ë¡œí•‘ ê²°ê³¼ ìˆ˜ì§‘\"\"\"\n    crops = db.query(CropResult).filter(CropResult.analysis_id == analysis_id).all()\n    \n    detections = []\n    for crop in crops:\n        if crop.detected_class and crop.confidence:\n            # í¬ë¡­ ê²°ê³¼ë¥¼ ê²€ì¶œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n            detection = {\n                'class_name': crop.detected_class,\n                'confidence': crop.confidence,\n                'bbox': [0, 0, 100, 100],  # ì„ì‹œ ì¢Œí‘œ\n                'area': crop.area,\n                'pnu': crop.pnu,\n                'source_type': 'crop'\n            }\n            detections.append(detection)\n    \n    return detections\n\n\n@app.get(\"/api/v1/exports/{export_id}\", response_model=APIResponse)\nasync def get_export(export_id: str, db: Session = Depends(get_db)):\n    \"\"\"ë‚´ë³´ë‚´ê¸° ìƒì„¸ ì¡°íšŒ\"\"\"\n    try:\n        export = db.query(Export).filter(Export.id == export_id).first()\n        if not export:\n            raise HTTPException(status_code=404, detail=\"ë‚´ë³´ë‚´ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        export_data = ExportResponse(\n            id=export.id,\n            name=export.name,\n            export_type=export.export_type,\n            status=export.status,\n            file_path=export.file_path,\n            filename=Path(export.file_path).name if export.file_path else None,\n            file_size=export.file_size,\n            layer_count=export.layer_count,\n            download_count=export.download_count,\n            created_at=export.created_at,\n            completed_at=export.completed_at,\n            expires_at=export.expires_at\n        )\n        \n        return APIResponse(\n            success=True,\n            message=\"ë‚´ë³´ë‚´ê¸° ì¡°íšŒ ì™„ë£Œ\",\n            data=export_data.dict()\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"ë‚´ë³´ë‚´ê¸° ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë‚´ë³´ë‚´ê¸° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@app.get(\"/api/v1/exports/{export_id}/download\")\nasync def download_export(export_id: str, db: Session = Depends(get_db)):\n    \"\"\"ë‚´ë³´ë‚´ê¸° íŒŒì¼ ë‹¤ìš´ë¡œë“œ\"\"\"\n    try:\n        export = db.query(Export).filter(Export.id == export_id).first()\n        if not export:\n            raise HTTPException(status_code=404, detail=\"ë‚´ë³´ë‚´ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if export.status != \"completed\":\n            raise HTTPException(status_code=400, detail=\"ì™„ë£Œëœ ë‚´ë³´ë‚´ê¸°ë§Œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n        \n        if not export.file_path or not Path(export.file_path).exists():\n            raise HTTPException(status_code=404, detail=\"ë‚´ë³´ë‚´ê¸° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # ë§Œë£Œì¼ í™•ì¸\n        if export.expires_at and datetime.utcnow() > export.expires_at:\n            raise HTTPException(status_code=410, detail=\"ë‚´ë³´ë‚´ê¸° íŒŒì¼ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤\")\n        \n        # ë‹¤ìš´ë¡œë“œ íšŸìˆ˜ ì¦ê°€\n        export.download_count += 1\n        db.commit()\n        \n        filename = Path(export.file_path).name\n        media_type = \"application/geopackage+sqlite3\" if export.export_type == \"gpkg\" else \"application/octet-stream\"\n        \n        return FileResponse(\n            path=export.file_path,\n            filename=filename,\n            media_type=media_type\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"ë‚´ë³´ë‚´ê¸° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë‚´ë³´ë‚´ê¸° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n\nprint(\"âœ… Exports API v2.0 êµ¬í˜„ ì™„ë£Œ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5jx0sj5aonw",
   "source": "## ğŸ“Š 12. Statistics API v2.0",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ryb2czk1rn",
   "source": "# Statistics API v2.0\n@app.get(\"/api/v1/statistics\", response_model=APIResponse)\nasync def get_statistics(db: Session = Depends(get_db)):\n    \"\"\"ì „ì²´ í†µê³„ ì¡°íšŒ\"\"\"\n    try:\n        # ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘\n        total_images = db.query(Image).count()\n        total_analyses = db.query(Analysis).count()\n        total_exports = db.query(Export).count()\n        \n        # ê²€ì¶œ ìˆ˜ ê³„ì‚°\n        total_detections = db.query(Analysis).filter(\n            Analysis.status == \"completed\"\n        ).with_entities(\n            db.func.sum(Analysis.total_detections)\n        ).scalar() or 0\n        \n        # ë¶„ì„ ìœ í˜•ë³„ í†µê³„\n        analysis_by_type = {}\n        type_stats = db.query(\n            Analysis.analysis_type,\n            db.func.count(Analysis.id)\n        ).group_by(Analysis.analysis_type).all()\n        \n        for analysis_type, count in type_stats:\n            analysis_by_type[analysis_type] = count\n        \n        # ê²€ì¶œ í´ë˜ìŠ¤ë³„ í†µê³„ (í¬ë¡­ ê²°ê³¼ ê¸°ë°˜)\n        detections_by_class = {}\n        class_stats = db.query(\n            CropResult.detected_class,\n            db.func.count(CropResult.id)\n        ).filter(\n            CropResult.detected_class.isnot(None)\n        ).group_by(CropResult.detected_class).all()\n        \n        for class_name, count in class_stats:\n            detections_by_class[class_name] = count\n        \n        # í‰ê·  ì²˜ë¦¬ ì‹œê°„\n        avg_processing_time = db.query(\n            db.func.avg(Analysis.processing_time)\n        ).filter(\n            Analysis.status == \"completed\",\n            Analysis.processing_time.isnot(None)\n        ).scalar() or 0.0\n        \n        stats_data = StatisticsResponse(\n            total_images=total_images,\n            total_analyses=total_analyses,\n            total_detections=int(total_detections),\n            total_exports=total_exports,\n            analysis_by_type=analysis_by_type,\n            detections_by_class=detections_by_class,\n            avg_processing_time=float(avg_processing_time),\n            last_updated=datetime.utcnow()\n        )\n        \n        return APIResponse(\n            success=True,\n            message=\"í†µê³„ ì¡°íšŒ ì™„ë£Œ\",\n            data=stats_data.dict()\n        )\n        \n    except Exception as e:\n        logger.error(f\"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@app.get(\"/api/v1/statistics/analyses/{analysis_id}\", response_model=APIResponse)\nasync def get_analysis_statistics(analysis_id: str, db: Session = Depends(get_db)):\n    \"\"\"ë¶„ì„ë³„ ìƒì„¸ í†µê³„\"\"\"\n    try:\n        # ë¶„ì„ ì¡´ì¬ í™•ì¸\n        analysis = db.query(Analysis).filter(Analysis.id == analysis_id).first()\n        if not analysis:\n            raise HTTPException(status_code=404, detail=\"ë¶„ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        stats = {\n            'analysis_id': analysis_id,\n            'analysis_name': analysis.name,\n            'analysis_type': analysis.analysis_type,\n            'status': analysis.status,\n            'total_detections': analysis.total_detections,\n            'total_tiles': analysis.total_tiles,\n            'processing_time': analysis.processing_time,\n            'created_at': analysis.created_at.isoformat(),\n            'completed_at': analysis.completed_at.isoformat() if analysis.completed_at else None\n        }\n        \n        if analysis.use_tiling:\n            # íƒ€ì¼ í†µê³„\n            tile_count = db.query(Tile).filter(Tile.analysis_id == analysis_id).count()\n            processed_tiles = db.query(Tile).filter(\n                Tile.analysis_id == analysis_id,\n                Tile.processed == True\n            ).count()\n            \n            stats['tile_statistics'] = {\n                'total_tiles': tile_count,\n                'processed_tiles': processed_tiles,\n                'processing_rate': processed_tiles / tile_count if tile_count > 0 else 0\n            }\n        else:\n            # í¬ë¡­ í†µê³„\n            crop_count = db.query(CropResult).filter(CropResult.analysis_id == analysis_id).count()\n            detected_crops = db.query(CropResult).filter(\n                CropResult.analysis_id == analysis_id,\n                CropResult.detected_class.isnot(None)\n            ).count()\n            \n            # í´ë˜ìŠ¤ë³„ í†µê³„\n            class_stats = db.query(\n                CropResult.detected_class,\n                db.func.count(CropResult.id),\n                db.func.avg(CropResult.confidence)\n            ).filter(\n                CropResult.analysis_id == analysis_id,\n                CropResult.detected_class.isnot(None)\n            ).group_by(CropResult.detected_class).all()\n            \n            class_breakdown = {}\n            for class_name, count, avg_conf in class_stats:\n                class_breakdown[class_name] = {\n                    'count': count,\n                    'avg_confidence': round(float(avg_conf), 3)\n                }\n            \n            stats['crop_statistics'] = {\n                'total_crops': crop_count,\n                'detected_crops': detected_crops,\n                'detection_rate': detected_crops / crop_count if crop_count > 0 else 0,\n                'class_breakdown': class_breakdown\n            }\n        \n        return APIResponse(\n            success=True,\n            message=\"ë¶„ì„ í†µê³„ ì¡°íšŒ ì™„ë£Œ\",\n            data=stats\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"ë¶„ì„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë¶„ì„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\nprint(\"âœ… Statistics API v2.0 êµ¬í˜„ ì™„ë£Œ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kx5j230ilsa",
   "source": "## ğŸš€ 13. ì„œë²„ ì‹œì‘ ë° í…ŒìŠ¤íŠ¸ (v2.0)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zfdkiede72",
   "source": "# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\ndef create_sample_data():\n    \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\"\"\"\n    db = SessionLocal()\n    \n    try:\n        # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±\n        sample_image = Image(\n            filename=\"sample_namwon_20250115.tif\",\n            file_path=\"./uploads/sample_namwon_20250115.tif\",\n            file_size=1024000,\n            format=\"tif\",\n            status=\"ready\",\n            description=\"ë‚¨ì›ì‹œ ë†ì—…ì§€ì—­ ë“œë¡  ì˜ìƒ ìƒ˜í”Œ\",\n            region_name=\"ì „ë¶ ë‚¨ì›ì‹œ\",\n            owner_id=test_user.id,\n            crs=\"EPSG:5186\",\n            bounds={\n                \"minx\": 200000.0,\n                \"miny\": 400000.0,\n                \"maxx\": 205000.0,\n                \"maxy\": 405000.0\n            },\n            resolution=0.25,\n            width=20000,\n            height=20000,\n            bands=3\n        )\n        \n        # ê¸°ì¡´ ìƒ˜í”Œ í™•ì¸\n        existing_sample = db.query(Image).filter(\n            Image.filename == sample_image.filename\n        ).first()\n        \n        if not existing_sample:\n            db.add(sample_image)\n            db.commit()\n            db.refresh(sample_image)\n            print(f\"âœ… ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±: {sample_image.id}\")\n        else:\n            sample_image = existing_sample\n            print(f\"â„¹ï¸ ìƒ˜í”Œ ì´ë¯¸ì§€ ì¡´ì¬: {sample_image.id}\")\n        \n        return sample_image\n        \n    except Exception as e:\n        logger.error(f\"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}\")\n        return None\n    finally:\n        db.close()\n\n\ndef test_apis():\n    \"\"\"API ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\"\"\"\n    print(\"\\nğŸ§ª API í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n    \n    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n    sample_image = create_sample_data()\n    if not sample_image:\n        print(\"âŒ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨\")\n        return\n    \n    print(f\"ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„°:\")\n    print(f\"   - ì´ë¯¸ì§€ ID: {sample_image.id}\")\n    print(f\"   - íŒŒì¼ëª…: {sample_image.filename}\")\n    print(f\"   - ì§€ì—­: {sample_image.region_name}\")\n    print(f\"   - í¬ê¸°: {sample_image.width}x{sample_image.height}\")\n    \n    # API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸\n    test_endpoints = [\n        {\n            \"name\": \"í—¬ìŠ¤ ì²´í¬\",\n            \"url\": \"/health\",\n            \"method\": \"GET\"\n        },\n        {\n            \"name\": \"ì´ë¯¸ì§€ ëª©ë¡\",\n            \"url\": \"/api/v1/images\",\n            \"method\": \"GET\"\n        },\n        {\n            \"name\": \"ì´ë¯¸ì§€ ìƒì„¸\",\n            \"url\": f\"/api/v1/images/{sample_image.id}\",\n            \"method\": \"GET\"\n        },\n        {\n            \"name\": \"ë¶„ì„ ëª©ë¡\",\n            \"url\": \"/api/v1/analyses\",\n            \"method\": \"GET\"\n        },\n        {\n            \"name\": \"ì „ì²´ í†µê³„\",\n            \"url\": \"/api/v1/statistics\",\n            \"method\": \"GET\"\n        }\n    ]\n    \n    print(f\"\\nğŸ”— í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸ ({len(test_endpoints)}ê°œ):\")\n    for endpoint in test_endpoints:\n        print(f\"   - {endpoint['method']} {endpoint['url']} ({endpoint['name']})\")\n    \n    print(\"\\nğŸ“– API ë¬¸ì„œ: http://localhost:8000/api/docs\")\n    print(\"ğŸ“˜ Redoc: http://localhost:8000/api/redoc\")\n\n\n# ì„œë²„ ì‹œì‘ í•¨ìˆ˜\ndef start_server():\n    \"\"\"ê°œë°œ ì„œë²„ ì‹œì‘\"\"\"\n    import uvicorn\n    \n    print(\"\\nğŸš€ Nong-View API ì„œë²„ ì‹œì‘ (v2.0)\")\n    print(f\"ğŸŒ ì£¼ì†Œ: http://{settings.HOST}:{settings.PORT}\")\n    print(f\"ğŸ“š API ë¬¸ì„œ: http://{settings.HOST}:{settings.PORT}/api/docs\")\n    print(f\"ğŸ”§ í™˜ê²½: {settings.ENVIRONMENT}\")\n    print(f\"ğŸ“Š ë²„ì „: {settings.VERSION}\")\n    \n    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° API ì •ë³´ ì¶œë ¥\n    test_apis()\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"ì„œë²„ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\")\n    print(\"start_server()\")\n    print(\"=\"*50)\n    \n    # ì‹¤ì œ ì„œë²„ ì‹œì‘ (ì£¼ì„ í•´ì œ ì‹œ ìë™ ì‹œì‘)\n    # uvicorn.run(\n    #     \"__main__:app\",\n    #     host=settings.HOST,\n    #     port=settings.PORT,\n    #     reload=settings.DEBUG\n    # )\n\n\n# ì„œë²„ ì •ë³´ ì¶œë ¥\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ‰ Nong-View v2.0 í†µí•© ë…¸íŠ¸ë¶ ì¤€ë¹„ ì™„ë£Œ!\")\nprint(\"=\"*60)\nprint(f\"ğŸ“ˆ ì™„ì„±ë„: 98% (v1.0 ê¸°ëŠ¥ + v2.0 ì‹ ê·œ ê¸°ëŠ¥)\")\nprint(f\"ğŸ†• v2.0 ì‹ ê·œ ê¸°ëŠ¥:\")\nprint(f\"   - âœ… POD3: ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ íƒ€ì¼ë§ ì‹œìŠ¤í…œ\")\nprint(f\"   - âœ… POD5: IoU ê¸°ë°˜ ì¤‘ë³µ ê²€ì¶œ ë³‘í•©\")\nprint(f\"   - âœ… POD6: GPKG í‘œì¤€ íŒŒì¼ ë‚´ë³´ë‚´ê¸°\")\nprint(f\"   - âœ… Analysis API v2.0 (íƒ€ì¼ë§ ì§€ì›)\")\nprint(f\"   - âœ… Crops API v2.0\")\nprint(f\"   - âœ… Exports API v2.0\")\nprint(f\"   - âœ… Statistics API v2.0\")\nprint(\"\\nğŸ”§ ì‹œì‘ ë°©ë²•:\")\nprint(\"   1. ëª¨ë“  ì…€ ì‹¤í–‰\")\nprint(\"   2. start_server() í•¨ìˆ˜ í˜¸ì¶œ\")\nprint(\"   3. http://localhost:8000/api/docs ì ‘ì†\")\nprint(\"\\nğŸ“š ë¬¸ì„œ:\")\nprint(\"   - API ë¬¸ì„œ: /api/docs\")\nprint(\"   - Redoc: /api/redoc\")\nprint(\"   - í—¬ìŠ¤ ì²´í¬: /health\")\nprint(\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\nprint(\"   - JWT ì¸ì¦ ì‹œìŠ¤í…œ êµ¬í˜„\")\nprint(\"   - í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±\")\nprint(\"   - í”„ë¡œë•ì…˜ ë°°í¬\")\nprint(\"=\"*60)\n\n# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\ntest_apis()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}